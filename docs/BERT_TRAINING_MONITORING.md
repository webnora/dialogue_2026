# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è BERT: –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ

**–î–∞—Ç–∞ –∑–∞–ø—É—Å–∫–∞**: 1 —Ñ–µ–≤—Ä–∞–ª—è 2026, 12:52
**–ü—Ä–æ—Ü–µ—Å—Å**: PID 11791
**–ö–æ—Ä–ø—É—Å**: `cleaned_guardian_filtered.csv` (48,140 —Ç–µ–∫—Å—Ç–æ–≤)
**–û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è**: ~2.5-3 —á–∞—Å–∞

---

## üöÄ –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞

### 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –∂–∏–≤

```bash
ps aux | grep "run_bert_training.py" | grep -v grep
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**:
```
nora  11791  X.X  X.X  427544832  XXXXXX  ??  UN  12:52PM   X:XX.XX python notebooks/run_bert_training.py
```

–ï—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –µ—Å—Ç—å ‚Üí –≤—Å—ë –∏–¥—ë—Ç ‚úÖ
–ï—Å–ª–∏ –ø—É—Å—Ç–æ ‚Üí –ø—Ä–æ—Ü–µ—Å—Å —É–º–µ—Ä ‚ùå (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥ –Ω–∞ –æ—à–∏–±–∫–∏)

---

### 2. –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å (–æ–¥–Ω–∞ –∫–æ–º–∞–Ω–¥–∞)

```bash
tail -20 results/bert_training_log.txt
```

**–ß—Ç–æ –∏—Å–∫–∞—Ç—å –≤ –≤—ã–≤–æ–¥–µ**:
- `Epoch 1/3`, `Epoch 2/3`, `Epoch 3/3` - –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏
- `Training: XX%` - –ø—Ä–æ–≥—Ä–µ—Å—Å –≤–Ω—É—Ç—Ä–∏ —ç–ø–æ—Ö–∏
- `loss=X.XXXX` - —Ç–µ–∫—É—â–∏–π loss (–¥–æ–ª–∂–µ–Ω —É–º–µ–Ω—å—à–∞—Ç—å—Å—è)
- `Val Accuracy: X.XXXX` - —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ø–æ—è–≤–ª—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏)

---

## üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### –í–∞—Ä–∏–∞–Ω—Ç A: –°–ª–µ–¥–∏—Ç—å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é)

```bash
tail -f results/bert_training_log.txt
```

**–ö–∞–∫ —á–∏—Ç–∞—Ç—å –≤—ã–≤–æ–¥**:
- **Training phase**: `Training: 25%|‚ñà‚ñà‚ñà‚ñà‚ñå         | 602/2407` (25% —ç–ø–æ—Ö–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ)
- **Loss**: `loss=0.5234, avg_loss=0.6123` (—Ç–µ–∫—É—â–∏–π –∏ —Å—Ä–µ–¥–Ω–∏–π loss)
- **Validation**: `Val Accuracy: 0.8765` (–ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏)
- **Epoch complete**: `‚úì New best model! Saving...` (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏)

**–ù–∞–∂–º–∏—Ç–µ `Ctrl+C` —á—Ç–æ–±—ã –≤—ã–π—Ç–∏ –∏–∑ —Ä–µ–∂–∏–º–∞ follow** (—ç—Ç–æ –ù–ï –ø—Ä–µ—Ä–≤—ë—Ç –æ–±—É—á–µ–Ω–∏–µ!)

---

### –í–∞—Ä–∏–∞–Ω—Ç B: –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞

```bash
# –ö–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å—Ç—Ä–æ–∫ –ª–æ–≥–∞
watch -n 60 'tail -5 results/bert_training_log.txt'
```

**–ù–∞–∂–º–∏—Ç–µ `Ctrl+C` —á—Ç–æ–±—ã –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å watch**

---

### –í–∞—Ä–∏–∞–Ω—Ç C: –ü—Ä–æ–≤–µ—Ä—è—Ç—å —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏

```bash
# –ü–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ —ç–ø–æ—Ö–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç—å
grep -E "Epoch|Val Accuracy|Testing|Final" results/bert_training_log.txt | tail -10
```

---

## üîç –ü–æ–Ω–∏–º–∞–Ω–∏–µ —ç—Ç–∞–ø–æ–≤ –æ–±—É—á–µ–Ω–∏—è

### –≠—Ç–∞–ø 1: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (2-3 –º–∏–Ω—É—Ç—ã)
```
[6/8] Tokenizing...
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38512/38512 [04:05<00:00, 157.43 examples/s]
```
- –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–æ–ª–∂–µ–Ω –¥–æ–π—Ç–∏ –¥–æ 100%
- 38,512 train + 4,814 val + 4,814 test = 48,140 —Ç–µ–∫—Å—Ç–æ–≤

---

### –≠—Ç–∞–ø 2: –û–±—É—á–µ–Ω–∏–µ (2-2.5 —á–∞—Å–∞)

```
Epoch 1/3
Training: 10%|‚ñà‚ñà‚ñà‚ñà         | 241/2407 [05:03<45:12, 1.27s/it]
```

**–ö–∞–∫ —á–∏—Ç–∞—Ç—å**:
- `10%` - 10% —ç–ø–æ—Ö–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ
- `241/2407` - batch 241 –∏–∑ 2407
- `[05:03<45:12]` - –ø—Ä–æ—à–ª–æ 5 –º–∏–Ω, –æ—Å—Ç–∞–ª–æ—Å—å 45 –º–∏–Ω
- `1.27s/it` - 1.27 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ batch

**–•–æ—Ä–æ—à–∏–π –∑–Ω–∞–∫** ‚Üí loss –ø–∞–¥–∞–µ—Ç (1.68 ‚Üí 1.20 ‚Üí 0.85 ‚Üí 0.50 ‚Üí ...)
**–ü–ª–æ—Ö–æ–π –∑–Ω–∞–∫** ‚Üí loss —Ä–∞—Å—Ç—ë—Ç –∏–ª–∏ NaN

---

### –≠—Ç–∞–ø 3: –í–∞–ª–∏–¥–∞—Ü–∏—è (–∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É)

```
Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 301/301 [01:23<00:00,  3.61s/it]

  Train Loss: 0.4523
  Val Accuracy: 0.8756
  Val F1 (macro): 0.8765

  ‚úì New best model! Saving...
```

- Val Accuracy –¥–æ–ª–∂–µ–Ω —Ä–∞—Å—Ç–∏ –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
- –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ accuracy —É–ª—É—á—à–∏–ª—Å—è

---

### –≠—Ç–∞–ø 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–≤ —Å–∞–º–æ–º –∫–æ–Ω—Ü–µ)

```
================================================================================
Testing on hold-out set...
================================================================================

Final Test Results:
  Accuracy: 0.8764
  F1 (macro): 0.8771
  F1 (weighted): 0.8771
```

**–ö–æ–≥–¥–∞ —É–≤–∏–¥–∏—Ç–µ "Testing"** ‚Üí –æ—Å—Ç–∞–ª–æ—Å—å 1-2 –º–∏–Ω—É—Ç—ã! üéâ

---

## üìÅ –§–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–∑–¥–∞—é—Ç—Å—è

### –í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è:
```
results/bert_training_log.txt          # –õ–æ–≥ –æ–±—É—á–µ–Ω–∏—è (—Ä–∞—Å—Ç—ë—Ç –¥–æ ~500KB-1MB)
models/bert_label_encoder_cleaned.pkl  # Label encoder (—Å–æ–∑–¥–∞—ë—Ç—Å—è —Å—Ä–∞–∑—É)
```

### –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è:
```
models/bert_category_classifier_cleaned/
‚îú‚îÄ‚îÄ config.json              # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
‚îú‚îÄ‚îÄ model.safetensors        # –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ (~437MB)
‚îú‚îÄ‚îÄ tokenizer_config.json    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
‚îú‚îÄ‚îÄ tokenizer.json           # –°–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
‚îî‚îÄ‚îÄ vocab.txt                # –°–ª–æ–≤–∞—Ä—å BERT (30K —Å–ª–æ–≤)

results/bert_metrics_cleaned.json           # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
results/bert_confusion_matrix_cleaned.npy   # Confusion matrix
```

**–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏**:
```bash
ls -lh models/bert_category_classifier_cleaned/
```

–ï—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ‚Üí –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! ‚úÖ

---

## ‚ö†Ô∏è –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º

### –ü—Ä–æ–±–ª–µ–º–∞ 1: –ü—Ä–æ—Ü–µ—Å—Å —É–º–µ—Ä

**–°–∏–º–ø—Ç–æ–º—ã**:
```bash
ps aux | grep run_bert_training  # –ü—É—Å—Ç–æ–π –≤—ã–≤–æ–¥
```

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å**:
```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω–µ—Ü –ª–æ–≥–∞ –Ω–∞ –æ—à–∏–±–∫–∏
tail -50 results/bert_training_log.txt

# 2. –ü–æ–∏—Å–∫ –æ—à–∏–±–æ–∫ Python
grep -i "error\|exception\|traceback" results/bert_training_log.txt

# 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞–º—è—Ç—å/–¥–∏—Å–∫
df -h  # –ú–µ—Å—Ç–æ –Ω–∞ –¥–∏—Å–∫–µ (–Ω—É–∂–Ω–æ >5GB —Å–≤–æ–±–æ–¥–Ω—ã—Ö)
```

**–ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏**:
- `OutOfMemoryError` ‚Üí –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ RAM
- `CUDA out of memory` ‚Üí GPU –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∞ (—É–º–µ–Ω—å—à–∏—Ç–µ BATCH_SIZE)
- `FileNotFoundError` ‚Üí –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º

---

### –ü—Ä–æ–±–ª–µ–º–∞ 2: Loss –Ω–µ –ø–∞–¥–∞–µ—Ç

**–°–∏–º–ø—Ç–æ–º—ã**: loss –æ—Å—Ç–∞—ë—Ç—Å—è –≤—ã—Å–æ–∫–∏–º (1.5-1.7) –∏–ª–∏ —Ä–∞—Å—Ç—ë—Ç

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å**:
```bash
# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –¥–∏–Ω–∞–º–∏–∫—É loss –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 batch
grep "loss=" results/bert_training_log.txt | tail -100
```

**–ù–æ—Ä–º–∞–ª—å–Ω–æ**: loss –ø–ª–∞–≤–Ω–æ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è (1.68 ‚Üí 1.50 ‚Üí 1.30 ‚Üí ...)
**–ü–ª–æ—Ö–æ**: loss –ø—Ä—ã–≥–∞–µ—Ç —Ö–∞–æ—Ç–∏—á–Ω–æ –∏–ª–∏ —Ä–∞—Å—Ç—ë—Ç

**–†–µ—à–µ–Ω–∏—è**:
- –ü–æ–¥–æ–∂–¥–∞—Ç—å –¥–æ –∫–æ–Ω—Ü–∞ —ç–ø–æ—Ö–∏ (–∏–Ω–æ–≥–¥–∞ improves –≤ –∫–æ–Ω—Ü–µ)
- –ï—Å–ª–∏ –ø–æ—Å–ª–µ 3-—Ö —ç–ø–æ—Ö loss > 1.0 ‚Üí –ø—Ä–æ–±–ª–µ–º–∞ —Å –¥–∞–Ω–Ω—ã–º–∏/–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏

---

### –ü—Ä–æ–±–ª–µ–º–∞ 3: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–∏—Å–ª–æ

**–°–∏–º–ø—Ç–æ–º—ã**: –õ–æ–≥ –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è >5 –º–∏–Ω—É—Ç

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ª–æ–≥–∞
ls -l results/bert_training_log.txt

# 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –Ω–µ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ D (uninterruptible sleep)
ps aux | grep run_bert_training
```

**–°—Ç–∞—Ç—É—Å—ã –ø—Ä–æ—Ü–µ—Å—Å–∞**:
- `R`/`RN` ‚Üí Running, –≤—Å—ë –æ–∫ ‚úÖ
- `S`/`SN` ‚Üí Sleeping (I/O), –Ω–æ—Ä–º–∞–ª—å–Ω–æ ‚úÖ
- `D` ‚Üí Uninterruptible sleep, –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ ‚ö†Ô∏è
- `Z` ‚Üí Zombie, —É–º–µ—Ä ‚ö†Ô∏è

---

## üìà –†–∞—Å—á—ë—Ç –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è

### –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏

```python
# –í Python-shell:
import re

with open('results/bert_training_log.txt') as f:
    content = f.read()

# –ù–∞–π—Ç–∏ —Ç–µ–∫—É—â–∏–π batch
matches = re.findall(r'(\d+)/2407', content)
if matches:
    current_batch = int(matches[-1])
    total_batches = 2407 * 3  # 3 —ç–ø–æ—Ö–∏
    progress = current_batch / total_batches

    print(f"–¢–µ–∫—É—â–∏–π batch: {current_batch}")
    print(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress*100:.1f}%")
    print(f"–û—Å—Ç–∞–ª–æ—Å—å: {(1-progress)*100:.1f}%")
```

---

### –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞—Å—á—ë—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏

```bash
# –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: 12:52
# –¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: —Å–µ–π—á–∞—Å
# –≠–ø–æ—Ö: 3
# Batch –Ω–∞ —ç–ø–æ—Ö—É: 2407
# –°–∫–æ—Ä–æ—Å—Ç—å: ~1.3s/batch

# –û–±—â–µ–µ –≤—Ä–µ–º—è = 2407 √ó 3 √ó 1.3 / 3600 = ~2.6 —á–∞—Å–∞
# –§–∏–Ω–∏—à –ø—Ä–∏–º–µ—Ä–Ω–æ: 12:52 + 2—á 36–º–∏–Ω = 15:28
```

---

## üéØ –ß—Ç–æ signalling –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ

### –£—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ ‚úÖ

–í –ª–æ–≥–µ —É–≤–∏–¥–∏—Ç–µ:
```
================================================================================
Testing on hold-out set...
================================================================================

Final Test Results:
  Accuracy: 0.XXXX
  F1 (macro): 0.XXXX
  F1 (weighted): 0.XXXX

================================================================================
Results saved:
  Model: models/bert_category_classifier_cleaned/
  Metrics: results/bert_metrics_cleaned.json
  Confusion Matrix: results/bert_confusion_matrix_cleaned.npy
  Label Encoder: models/bert_label_encoder_cleaned.pkl
================================================================================
```

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
```bash
# 1. –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à—ë–Ω
ps aux | grep run_bert_training  # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç–æ–π

# 2. –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞
ls -lh models/bert_category_classifier_cleaned/

# 3. –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–ø–∏—Å–∞–Ω—ã
cat results/bert_metrics_cleaned.json
```

---

### –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–æ–π ‚ùå

**–°–∏–º–ø—Ç–æ–º—ã**:
- –ü—Ä–æ—Ü–µ—Å—Å —É–º–µ—Ä
- –í –ª–æ–≥–µ –µ—Å—Ç—å `Traceback` –∏–ª–∏ `Error`

**–ß—Ç–æ –¥–µ–ª–∞—Ç—å**:
```bash
# 1. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–≥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
cp results/bert_training_log.txt results/bert_training_log_FAILED.txt

# 2. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ
python3 notebooks/run_bert_training.py > results/bert_training_log.txt 2>&1 &
```

---

## üõ†Ô∏è –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã (cheat sheet)

```bash
# === –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ===
# –ü—Ä–æ—Ü–µ—Å—Å –∂–∏–≤?
ps aux | grep run_bert_training | grep -v grep

# –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Å—Ç—Ä–æ–∫ –ª–æ–≥–∞
tail -20 results/bert_training_log.txt

# === –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ ===
# –°–ª–µ–¥–∏—Ç—å –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
tail -f results/bert_training_log.txt

# –ü—Ä–æ–≤–µ—Ä—è—Ç—å –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
watch -n 30 'tail -10 results/bert_training_log.txt'

# –¢–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫–∏
grep -E "Epoch|Val Accuracy|Loss" results/bert_training_log.txt | tail -20

# === –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ ===
# –†–∞–∑–º–µ—Ä –ª–æ–≥–∞
ls -lh results/bert_training_log.txt

# –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞?
ls -la models/bert_category_classifier_cleaned/

# –ù–∞–π—Ç–∏ –æ—à–∏–±–∫–∏
grep -i "error\|exception" results/bert_training_log.txt

# === –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ===
# –°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –≤ –ª–æ–≥–µ
wc -l results/bert_training_log.txt

# –ù–∞–π—Ç–∏ –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —ç–ø–æ—Ö
grep "Epoch" results/bert_training_log.txt
```

---

## üìû –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫

### –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–π—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å! –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ:

1. **–ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –ª–æ–≥ –ø–æ–ª–Ω–æ—Å—Ç—å—é**:
   ```bash
   less results/bert_training_log.txt
   # –ù–∞–≤–∏–≥–∞—Ü–∏—è: ‚Üì –≤–≤–µ—Ä—Ö, ‚Üì –≤–Ω–∏–∑, /pattern –ø–æ–∏—Å–∫, q –≤—ã—Ö–æ–¥
   ```

2. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∏—Å–∫**:
   ```bash
   df -h
   # –ù—É–∂–Ω–æ >5GB —Å–≤–æ–±–æ–¥–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
   ```

3. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ RAM**:
   ```bash
   vm_stat | head -10
   # –ò–ª–∏ —á–µ—Ä–µ–∑ Activity Monitor
   ```

4. **–ï—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–∏—Å (D-—Å–æ—Å—Ç–æ—è–Ω–∏–µ >10 –º–∏–Ω)**:
   ```bash
   # –ú–æ–∂–Ω–æ —É–±–∏—Ç—å, –Ω–æ –ª—É—á—à–µ –¥–æ–∂–¥–∞—Ç—å—Å—è
   kill -9 11791  # PID –ø—Ä–æ—Ü–µ—Å—Å–∞
   ```

---

## üìù –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è

### 1. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

```bash
# –ú–µ—Ç—Ä–∏–∫–∏
cat results/bert_metrics_cleaned.json

# –û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:
{
  "test_accuracy": 0.8764,
  "test_f1_macro": 0.8771,
  "test_f1_weighted": 0.8771,
  "best_val_accuracy": 0.8800,
  "categories": ["Analytical", "Editorial", "Feature", "News", "Review"],
  ...
}
```

---

### 2. –°—Ä–∞–≤–Ω–∏—Ç—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏

```bash
# –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏
ls results/*metrics*.json

# TF-IDF baseline
cat results/tfidf_metrics.json

# BERT (—Å—Ç–∞—Ä—ã–π)
cat results/bert_metrics.json

# BERT (–Ω–æ–≤—ã–π, filtered)
cat results/bert_metrics_cleaned.json
```

---

### 3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

```bash
python3 classify_texts.py \
  --model models/bert_category_classifier_cleaned \
  --label_encoder models/bert_label_encoder_cleaned.pkl \
  --text "Your text here..."
```

---

## üéì –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

### –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö

| –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ | –í—Ä–µ–º—è –Ω–∞ batch | –û–±—â–µ–µ –≤—Ä–µ–º—è |
|------------|---------------|-------------|
| MPS (M1/M2) | ~1.3s | ~2.5-3 —á–∞—Å–∞ |
| NVIDIA RTX 3090 | ~0.2s | ~25-30 –º–∏–Ω |
| CPU | ~8s | ~16-18 —á–∞—Å–æ–≤ |

**–í–∞—à —Ç–µ–∫—É—â–∏–π**: MPS (Apple Silicon) ‚úÖ

---

### –¢–∏–ø–∏—á–Ω—ã–π loss plot (–µ—Å–ª–∏ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫)

```
Loss
1.6 |‚óè
1.4 |  ‚óè
1.2 |    ‚óè
1.0 |      ‚óè
0.8 |        ‚óè‚óè
0.6 |          ‚óè‚óè‚óè
0.4 |            ‚óè‚óè‚óè‚óè‚óè
0.2 |________________‚óè‚óè
    0    1000   2000   3000  Batch
```

**–ù–æ—Ä–º–∞–ª—å–Ω–æ** ‚Üí –ø–ª–∞–≤–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ
**–ê–Ω–æ–º–∞–ª—å–Ω–æ** ‚Üí —Ä–µ–∑–∫–∏–µ —Å–∫–∞—á–∫–∏ –∏–ª–∏ —Ä–æ—Å—Ç

---

## ‚è∞ –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏

### –û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–æ–µ –≤—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è

```bash
# –¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è:
date

# –ü—Ä–∏–±–∞–≤–∏—Ç—å ~2.5 —á–∞—Å–∞
# –ü—Ä–∏–º–µ—Ä: –∑–∞–ø—É—Å—Ç–∏–ª–∏ –≤ 12:52 ‚Üí —Ñ–∏–Ω–∏—à –≤ 15:22
```

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
- üåô **–ó–∞–ø—É—Å–∫–∞—Ç—å –Ω–∞ –Ω–æ—á—å** ‚Üí —É—Ç—Ä–æ–º –≥–æ—Ç–æ–≤–æ
- ‚òï **–û—Ç–æ–π—Ç–∏ –Ω–∞ –∫–æ—Ñ–µ** ‚Üí 30 –º–∏–Ω –Ω–µ —Ö–≤–∞—Ç–∏—Ç
- üìö **–ó–∞–Ω—è—Ç—å—Å—è –¥—Ä—É–≥–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏** ‚Üí —Å–∞–º–æ–µ –ª—É—á—à–µ–µ!

---

## ‚úÖ –ß–µ–∫-–ª–∏—Å—Ç: –≤—Å—ë –ª–∏ –∏–¥—ë—Ç —Ö–æ—Ä–æ—à–æ?

- [ ] –ü—Ä–æ—Ü–µ—Å—Å –∂–∏–≤ (PID 11791)
- [ ] Loss –ø–∞–¥–∞–µ—Ç (–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–Ω–∞—á–µ–Ω–∏–π)
- [ ] –õ–æ–≥ —Ä–∞—Å—Ç—ë—Ç (`ls -lh results/bert_training_log.txt`)
- [ ] –ù–µ—Ç –æ—à–∏–±–æ–∫ –≤ –ª–æ–≥–µ
- [ ] RAM/–¥–∏—Å–∫ –≤ –Ω–æ—Ä–º–µ
- [ ] GPU/CPU —Ä–∞–±–æ—Ç–∞–µ—Ç

–ï—Å–ª–∏ –≤—Å—ë –≥–∞–ª–æ—á–∫–∞–º–∏ ‚Üí **–æ—Ç–¥—ã—Ö–∞–π—Ç–µ, –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è!** üéâ

---

**–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω**: 1 —Ñ–µ–≤—Ä–∞–ª—è 2026
**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ**: –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è
**–°—Ç–∞—Ç—É—Å**: ‚úÖ ACTIVE (PID 11791)

---

## üìö –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

- [HuggingFace BERT documentation](https://huggingface.co/docs/transformers/model_doc/bert)
- [PyTorch MPS backend](https://pytorch.org/docs/stable/mps.html)
- [Transformers training](https://huggingface.co/docs/transformers/training)

---

**–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ–≥–æ —Å—Ä–∞–∑—É**:

```bash
echo "=== –°—Ç–∞—Ç—É—Å –ø—Ä–æ—Ü–µ—Å—Å–∞ ===" && \
ps aux | grep run_bert_training | grep -v grep && \
echo "" && \
echo "=== –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å—Ç—Ä–æ–∫ –ª–æ–≥–∞ ===" && \
tail -5 results/bert_training_log.txt && \
echo "" && \
echo "=== –†–∞–∑–º–µ—Ä –ª–æ–≥–∞ ===" && \
ls -lh results/bert_training_log.txt && \
echo "" && \
echo "=== –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞? ===" && \
ls -la models/bert_category_classifier_cleaned/ 2>/dev/null || echo "–ï—â—ë –Ω–µ —Å–æ–∑–¥–∞–Ω–∞"
```

–°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∏ –≤—Å—Ç–∞–≤—å—Ç–µ –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª ‚Äî —É–≤–∏–¥–∏—Ç–µ –≤—Å—é –∫–∞—Ä—Ç–∏–Ω—É! üöÄ