# –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

**–ß—Ç–æ vector representations reveal about publicistic writing: learning from mistakes**

---

## –û–±–∑–æ—Ä

**–û–±—â–∏–π —Å—Ä–æ–∫**: 8‚Äì12 –Ω–µ–¥–µ–ª—å

**–¶–µ–ª—å**: –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—å—é –¥–ª—è –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ Dialogue –ø–æ –∂–∞–Ω—Ä–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—É–±–ª–∏—Ü–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫

**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è**: –û—à–∏–±–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—Ç—Ä–∞–∂–∞—é—Ç —Ä–µ–∞–ª—å–Ω—É—é –∂–∞–Ω—Ä–æ–≤—É—é –±–ª–∏–∑–æ—Å—Ç—å –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ—Å—Ç—å –∂–∞–Ω—Ä–æ–≤—ã—Ö –≥—Ä–∞–Ω–∏—Ü

---

## –§–∞–∑–∞ 1: –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (–ù–µ–¥–µ–ª–∏ 1‚Äì3)

### –ó–∞–¥–∞—á–∞ 1.1: TF‚ÄìIDF + Logistic Regression

**–°—Ä–æ–∫**: 2‚Äì3 –¥–Ω—è

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π notebook: `models/tfidf_baseline.ipynb`

2. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞–π–ø–ª–∞–π–Ω:
   ```python
   from sklearn.feature_extraction.text import TfidfVectorizer
   from sklearn.linear_model import LogisticRegression
   from sklearn.model_selection import GridSearchCV

   # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
   vectorizer = TfidfVectorizer(
       max_features=10000,
       ngram_range=(1, 2),
       min_df=5,
       max_df=0.8
   )

   # Grid search –¥–ª—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
   param_grid = {'C': [0.1, 1, 10, 100]}
   ```

3. –û–±—É—á–∏—Ç—å –Ω–∞ train, –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ validation, –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ test

4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å:
   - –ú–æ–¥–µ–ª—å: `models/tfidf_lr.pkl`
   - –ú–µ—Ç—Ä–∏–∫–∏: `results/tfidf_metrics.json`
   - Confusion matrix: `results/tfidf_confusion.npy`

5. –ò–∑–≤–ª–µ—á—å —Ç–æ–ø-50 —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∂–∞–Ω—Ä–∞ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏)

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- Accuracy: ~75%
- –°–ø–∏—Å–æ–∫ –ª–µ–∫—Å–∏—á–µ—Å–∫–∏—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∂–∞–Ω—Ä–∞

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞
- [x] –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
- [x] Confusion matrix –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞

---

### –ó–∞–¥–∞—á–∞ 1.2: –õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏

**–°—Ä–æ–∫**: 4‚Äì5 –¥–Ω–µ–π

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å notebook: `models/linguistic_features.ipynb`

2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å spaCy):
   ```python
   import spacy
   from collections import Counter

   nlp = spacy.load("en_core_web_sm")

   def extract_linguistic_features(text):
       doc = nlp(text)

       features = {
           # –õ–µ–∫—Å–∏–∫–æ-–≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ
           'type_token_ratio': len(set(doc)) / len(doc),
           'avg_sentence_length': np.mean([len(sent) for sent in doc.sents]),
           'modal_ratio': count_modals(doc) / len(doc),
           'first_person_ratio': count_pronouns(doc, person=1) / len(doc),
           'second_person_ratio': count_pronouns(doc, person=2) / len(doc),
           'third_person_ratio': count_pronouns(doc, person=3) / len(doc),

           # –î–∏—Å–∫—É—Ä—Å–∏–≤–Ω—ã–µ
           'stance_markers_ratio': count_stance_markers(doc) / len(doc),
           'hedges_ratio': count_hedges(doc) / len(doc),
           'quotes_ratio': text.count('"') / len(text),
       }

       return features
   ```

3. –°–æ–∑–¥–∞—Ç—å —Å–ø–∏—Å–∫–∏ stance markers –∏ hedges:
   ```python
   STANCE_MARKERS = [
       'arguably', 'reportedly', 'seemingly', 'apparently',
       'undoubtedly', 'clearly', 'obviously', 'evidently',
       'supposedly', 'presumably', 'ostensibly'
   ]

   HEDGES = [
       'perhaps', 'possibly', 'somewhat', 'rather',
       'quite', 'somewhat', 'relatively', 'comparatively'
   ]
   ```

4. –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∫–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º (—Å progress bar)

5. –û–±—É—á–∏—Ç—å Random Forest:
   ```python
   from sklearn.ensemble import RandomForestClassifier

   rf = RandomForestClassifier(
       n_estimators=200,
       max_depth=10,
       min_samples_split=5,
       random_state=42
   )
   ```

6. Feature importance: –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- Accuracy: ~78%
- –°–ø–∏—Å–æ–∫ discriminative –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã –±–µ–∑ –æ—à–∏–±–æ–∫
- [x] –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞
- [x] Feature importance –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω

**–ü—Ä–æ–±–ª–µ–º—ã**:
- spaCy –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–º –Ω–∞ 50K —Ç–µ–∫—Å—Ç–æ–≤ ‚Üí –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å multiprocessing

---

### –ó–∞–¥–∞—á–∞ 1.3: BERT fine-tuning

**–°—Ä–æ–∫**: 3‚Äì4 –¥–Ω—è

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å notebook: `models/bert_finetuning.ipynb`

2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–¥ –∏–∑ `alina/Obuchenie (2).ipynb` –∏–ª–∏:
   ```python
   from transformers import BertTokenizer, BertForSequenceClassification

   tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
   model = BertForSequenceClassification.from_pretrained(
       "bert-base-uncased",
       num_labels=5
   )
   ```

3. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
   - Learning rate: 2e-5
   - Epochs: 3
   - Batch size: 16
   - Max length: 256

4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å:
   - –ú–æ–¥–µ–ª—å: `models/bert_category_classifier/`
   - –ú–µ—Ç—Ä–∏–∫–∏: `results/bert_metrics.json`
   - Confusion matrix: `results/bert_confusion_matrix.npy`

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- Accuracy: ~87-88%

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] BERT –æ–±—É—á–µ–Ω
- [x] –ú–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ TF-IDF (—Ö–æ—Ç—è –±—ã –Ω–∞ 0.5%)

**–ü—Ä–æ–±–ª–µ–º—ã**:
- BERT —Ç—Ä–µ–±—É–µ—Ç GPU ‚Üí –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Apple Silicon MPS –∏–ª–∏ Google Colab
- –ï—Å–ª–∏ –Ω–µ –≤–ª–µ–∑–∞–µ—Ç: —É–º–µ–Ω—å—à–∏—Ç—å batch size –¥–æ 8

---

### –ó–∞–¥–∞—á–∞ 1.4: –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

**–°—Ä–æ–∫**: 1 –¥–µ–Ω—å

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å notebook: `results/baseline_comparison.ipynb`

2. –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å:
   ```python
   results = {
       'TF-IDF + LR': load_metrics('results/tfidf_metrics.json'),
       'Linguistic + RF': load_metrics('results/linguistic_metrics.json'),
       'BERT': load_metrics('results/bert_metrics.json'),
   }

   comparison = pd.DataFrame(results).T
   ```

3. –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å:
   - Bar chart —Å accuracy –∏ F1
   - Confidence intervals (–ø–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∏)

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] –í—Å–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã
- [x] –¢–∞–±–ª–∏—Ü–∞ —Å–æ–∑–¥–∞–Ω–∞
- [x] BERT > TF‚ÄìIDF > Linguistic

---

## –§–∞–∑–∞ 2: –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ (–ù–µ–¥–µ–ª–∏ 4‚Äì6)

### –ó–∞–¥–∞—á–∞ 2.1: Confusion matrices –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π

**–°—Ä–æ–∫**: 2 –¥–Ω—è

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å notebook: `analysis/confusion_matrices.ipynb`

2. –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏:
   ```python
   from sklearn.metrics import confusion_matrix
   import seaborn as sns

   cm = confusion_matrix(y_true, y_pred)
   plt.figure(figsize=(10, 8))
   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')

   plt.savefig('results/confusion_{model_name}.png')
   ```

3. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å confusion matrix (–ø–æ —Å—Ç—Ä–æ–∫–∞–º):
   ```python
   cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
   ```

4. –°—Ä–∞–≤–Ω–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- 3 confusion matrices (—Å—ã—Ä—ã–µ + –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ)
- –í—ã—è–≤–ª–µ–Ω—ã –∂–∞–Ω—Ä–æ–≤—ã–µ –ø–∞—Ä—ã —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –ø—É—Ç–∞–Ω–∏—Ü–æ–π

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] –í—Å–µ –º–∞—Ç—Ä–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã
- [x] Identified top 3 error-prone genre pairs
–ø–æ
---

### –ó–∞–¥–∞—á–∞ 2.2: –í—ã—è–≤–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∂–∞–Ω—Ä–æ–≤—ã—Ö –ø–∞—Ä

**–°—Ä–æ–∫**: 2 –¥–Ω—è

**–®–∞–≥–∏**:

1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å confusion matrices

2. –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ–ø-3 –ø—É—Ç–∞—é—â–∏—Ö—Å—è –ø–∞—Ä:
   ```python
   def get_top_confusions(cm, labels, top_n=3):
       confusions = []
       for i in range(len(labels)):
           for j in range(len(labels)):
               if i != j:
                   confusions.append({
                       'pair': (labels[i], labels[j]),
                       'count': cm[i, j]
                   })

       return sorted(confusions, key=lambda x: -x['count'])[:top_n]
   ```

3. –°—Ä–∞–≤–Ω–∏—Ç—å —Å–ø–∏—Å–∫–∏ –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏:
   - –ö–∞–∫–∏–µ –ø–∞—Ä—ã —Å—Ç–∞–±–∏–ª—å–Ω–æ –ø—É—Ç–∞—é—Ç—Å—è?
   - –ï—Å—Ç—å –ª–∏ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏?

**–û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã**:
- News ‚Üî Analysis (12%)
- Editorial ‚Üî Review (9%)
- Feature ‚Üî Analysis (8%)

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] –¢–æ–ø-3 –ø–∞—Ä—ã –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã
- [x] –ï—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏

---

### –ó–∞–¥–∞—á–∞ 2.3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤

**–°—Ä–æ–∫**: 3 –¥–Ω—è

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å notebook: `analysis/error_examples.ipynb`

2. –î–ª—è –∫–∞–∂–¥–æ–π –∫–ª—é—á–µ–≤–æ–π –∂–∞–Ω—Ä–æ–≤–æ–π –ø–∞—Ä—ã:
   - –ù–∞–π—Ç–∏ 10‚Äì15 –ø—Ä–∏–º–µ—Ä–æ–≤, –≥–¥–µ **–≤—Å–µ** –º–æ–¥–µ–ª–∏ –æ—à–∏–±–∞—é—Ç—Å—è
   - –ù–∞–π—Ç–∏ 5‚Äì10 –ø—Ä–∏–º–µ—Ä–æ–≤, –≥–¥–µ **—Ç–æ–ª—å–∫–æ** BERT –æ—à–∏–±–∞–µ—Ç—Å—è

3. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã:
   ```python
   error_examples = {
       'news_analysis': {
           'all_wrong': [indices],
           'bert_wrong_only': [indices]
       },
       'editorial_review': {...},
       'feature_analysis': {...}
   }
   ```

4. –í—ã–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç—ã —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏:
   ```python
   def print_error_example(idx, text, true_label, predictions):
       print(f"True: {true_label}")
       for model, pred in predictions.items():
           print(f"{model}: {pred} {'‚úì' if pred == true_label else '‚úó'}")
       print(f"Text: {text[:500]}...")
       print("-" * 80)
   ```

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] –°–æ–±—Ä–∞–Ω–æ 30‚Äì50 –æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
- [x] –ü—Ä–∏–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ CSV/JSON

---

### –ó–∞–¥–∞—á–∞ 2.4: –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑

**–°—Ä–æ–∫**: 5‚Äì7 –¥–Ω–µ–π

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å document: `analysis/qualitative_analysis.md`

2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—à–∏–±–æ—á–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å:

   **–ú–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å**:
   - –ö–∞–∫–∏–µ –º–æ–¥–∞–ª—å–Ω—ã–µ –≥–ª–∞–≥–æ–ª—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è?
   - –ö–∞–∫ –≤—ã—Ä–∞–∂–µ–Ω–∞ –∞–≤—Ç–æ—Ä—Å–∫–∞—è –ø–æ–∑–∏—Ü–∏—è?
   - –ï—Å—Ç—å –ª–∏ hedges?

   **–î–∏—Å–∫—É—Ä—Å–∏–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã**:
   - Stance markers (arguably, reportedly, etc.)
   - –¶–∏—Ç–∞—Ç—ã –∏ reporting verbs
   - –ú–µ—Ç–∞—Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏

   **–°—Ç—Ä—É–∫—Ç—É—Ä–∞**:
   - –ù–∞—Ä—Ä–∞—Ç–∏–≤–Ω–∞—è –∏–ª–∏ –ª–æ–≥–∏—á–µ—Å–∫–∞—è?
   - –ï—Å—Ç—å –ª–∏ —è—Ä–∫–æ –≤—ã—Ä–∞–∂–µ–Ω–Ω–∞—è –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ü–∏—è?
   - –ù–∞–ª–∏—á–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤, –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π

   **–û—Ü–µ–Ω–æ—á–Ω–æ—Å—Ç—å**:
   - –û—Ü–µ–Ω–æ—á–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞ (excellent, terrible, etc.)
   - –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ–∫—Ä–∞—à–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞

3. –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã:
   - –ß—Ç–æ –æ–±—â–µ–≥–æ —É —Ç–µ–∫—Å—Ç–æ–≤, –ø—É—Ç–∞—é—â–∏—Ö News –∏ Analysis?
   - –ü–æ—á–µ–º—É Editorial –ø—É—Ç–∞–µ—Ç—Å—è —Å Review?

4. –°–≤—è–∑–∞—Ç—å —Å –∂–∞–Ω—Ä–æ–≤–æ–π —Ç–µ–æ—Ä–∏–µ–π:
   - Bhatia (1993) ‚Äî genre as rhetorical action
   - Swales (2004) ‚Äî genre as communicative event
   - –°—Ä–∞–≤–Ω–∏—Ç—å —Å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –æ–∂–∏–¥–∞–Ω–∏—è–º–∏

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- –î–æ–∫—É–º–µ–Ω—Ç 5‚Äì10 —Å—Ç—Ä–∞–Ω–∏—Ü —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
- 3‚Äì5 –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –º–∏–Ω–∏–º—É–º 15 –ø—Ä–∏–º–µ—Ä–æ–≤
- [x] –í—ã—è–≤–ª–µ–Ω—ã –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- [x] –°–≤—è–∑—å —Å —Ç–µ–æ—Ä–∏–µ–π —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞

---

## –§–∞–∑–∞ 3: –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è BERT (–ù–µ–¥–µ–ª–∏ 7‚Äì8)

### –ó–∞–¥–∞—á–∞ 3.1: Attention extraction

**–°—Ä–æ–∫**: 3‚Äì4 –¥–Ω—è

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å notebook: `analysis/attention_analysis.ipynb`

2. –ò–∑–≤–ª–µ—á—å attention weights –¥–ª—è [CLS] —Ç–æ–∫–µ–Ω–∞:
   ```python
   def extract_attention(model, tokenizer, text):
       inputs = tokenizer(text, return_tensors="pt")
       outputs = model(**inputs, output_attentions=True)

       # Average attention across all heads and layers
       attentions = torch.stack(outputs.attentions)  # (layers, batch, heads, seq, seq)
       cls_attention = attentions[:, :, :, 0, :]  # Attention to [CLS]
       avg_attention = cls_attention.mean(dim=(0, 1, 2))  # Average over layers, batch, heads

       return avg_attention
   ```

3. –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å top-attended words:
   ```python
   def visualize_attention(text, attention, tokenizer, top_n=10):
       tokens = tokenizer.tokenize(text)
       top_indices = attention.argsort()[-top_n:][::-1]

       for idx in top_indices:
           print(f"{tokens[idx]}: {attention[idx]:.3f}")
   ```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- Attention weights –¥–ª—è 20‚Äì30 –ø—Ä–∏–º–µ—Ä–æ–≤ (–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö + –æ—à–∏–±–æ—á–Ω—ã—Ö)

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] Attention –∏–∑–≤–ª–µ—á—ë–Ω
- [x] –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç

---

### –ó–∞–¥–∞—á–∞ 3.2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ attention patterns

**–°—Ä–æ–∫**: 2‚Äì3 –¥–Ω—è

**–®–∞–≥–∏**:

1. –°—Ä–∞–≤–Ω–∏—Ç—å attention –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤:
   - –ö–∞–∫–∏–µ —Å–ª–æ–≤–∞ —Ç–∏–ø–∏—á–Ω—ã –¥–ª—è News?
   - –ö–∞–∫–∏–µ –¥–ª—è Editorial?
   - –ö–∞–∫–∏–µ –¥–ª—è Review?

2. –î–ª—è –æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤:
   - –ù–∞ —á—Ç–æ –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç, –∫–æ–≥–¥–∞ –æ—à–∏–±–∞–µ—Ç—Å—è?
   - –ï—Å—Ç—å –ª–∏ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã?

3. –ü—Ä–∏–º–µ—Ä –≥–∏–ø–æ—Ç–µ–∑—ã:
   - –ï—Å–ª–∏ –ø—É—Ç–∞–µ—Ç News –∏ Analysis ‚Üí —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Ñ–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞, –Ω–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –¥–∏—Å–∫—É—Ä—Å–∏–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- –¢–∞–±–ª–∏—Ü–∞: —Ç–æ–ø-10 —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∂–∞–Ω—Ä–∞ –ø–æ attention
- –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –ø—Ä–∏–∑–º—É attention

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [x] –í—ã—è–≤–ª–µ–Ω—ã –∂–∞–Ω—Ä–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã attention
- [x] –û—à–∏–±–∫–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞–Ω—ã

---

### –ó–∞–¥–∞—á–∞ 3.3: SHAP/LIME (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

**–°—Ä–æ–∫**: 3‚Äì4 –¥–Ω—è (–µ—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º—è)

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å notebook: `analysis/shap_analysis.ipynb`

2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SHAP –¥–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:
   ```python
   import shap

   explainer = shap.Explainer(model, tokenizer)
   shap_values = explainer(texts)

   shap.plots.text(shap_values[0])
   ```

3. –°—Ä–∞–≤–Ω–∏—Ç—å SHAP values —Å attention:
   - –ï—Å—Ç—å –ª–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è?
   - –ß—Ç–æ –ª—É—á—à–µ –æ–±—ä—è—Å–Ω—è–µ—Ç –æ—à–∏–±–∫–∏?

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- SHAP visualizations –¥–ª—è 10‚Äì15 –ø—Ä–∏–º–µ—Ä–æ–≤

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [ ] SHAP —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ**: –ú–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å, –µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–∏

---

## –§–∞–∑–∞ 4: –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ (–ù–µ–¥–µ–ª–∏ 9‚Äì12)

### –ó–∞–¥–∞—á–∞ 4.1: McNemar's test

**–°—Ä–æ–∫**: 2 –¥–Ω—è

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å notebook: `analysis/statistical_tests.ipynb`

2. –°—Ä–∞–≤–Ω–∏—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ–ø–∞—Ä–Ω–æ:
   ```python
   from statsmodels.stats.contingency_tables import mcnemar

   def compare_models(y_true, y_pred1, y_pred2):
       # Create contingency table
       #        Model2 Correct | Model2 Wrong
       # Model1 Correct |     a      |      b
       # Model1 Wrong   |     c      |      d

       a = np.sum((y_pred1 == y_true) & (y_pred2 == y_true))
       b = np.sum((y_pred1 == y_true) & (y_pred2 != y_true))
       c = np.sum((y_pred1 != y_true) & (y_pred2 == y_true))
       d = np.sum((y_pred1 != y_true) & (y_pred2 != y_true))

       result = mcnemar([[a, b], [c, d]], exact=True)

       return result
   ```

3. –°—Ä–∞–≤–Ω–∏—Ç—å:
   - BERT vs TF‚ÄìIDF
   - BERT vs Linguistic
   - TF‚ÄìIDF vs Linguistic

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- p-values –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã
- –ö–∞–∫–∏–µ —Ä–∞–∑–ª–∏—á–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã?

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [ ] –í—Å–µ –ø–∞—Ä—ã —Å—Ä–∞–≤–Ω–µ–Ω—ã
- [ ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã

---

### –ó–∞–¥–∞—á–∞ 4.2: Bootstrap confidence intervals

**–°—Ä–æ–∫**: 2 –¥–Ω—è

**–®–∞–≥–∏**:

1. Bootstrap –¥–ª—è accuracy –∏ F1:
   ```python
   from sklearn.utils import resample

   def bootstrap_metric(y_true, y_pred, metric, n_iterations=1000):
       scores = []
       for _ in range(n_iterations):
           y_true_bs, y_pred_bs = resample(y_true, y_pred)
           score = metric(y_true_bs, y_pred_bs)
           scores.append(score)

       return np.mean(scores), np.percentile(scores, [2.5, 97.5])
   ```

2. –î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏:
   ```python
   mean, ci = bootstrap_metric(y_test, y_pred, accuracy_score)
   print(f"Accuracy: {mean:.3f} [{ci[0]:.3f}, {ci[1]:.3f}]")
   ```

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- –¢–∞–±–ª–∏—Ü–∞ —Å confidence intervals –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [ ] 95% CI –ø–æ—Å—á–∏—Ç–∞–Ω—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
- [ ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü—É

---

### –ó–∞–¥–∞—á–∞ 4.3: Inter-annotator agreement

**–°—Ä–æ–∫**: 3‚Äì4 –¥–Ω—è

**–®–∞–≥–∏**:

1. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –≤—ã–±–æ—Ä–∫—É:
   - –°–ª—É—á–∞–π–Ω—ã–µ 300 —Ç–µ–∫—Å—Ç–æ–≤
   - –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ CSV/Google Sheets

2. –ù–∞–π—Ç–∏ –≤—Ç–æ—Ä–æ–≥–æ –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞ (–∏–ª–∏ —Å–¥–µ–ª–∞—Ç—å —Å–∞–º–æ–º—É —Å –ø–µ—Ä–µ—Ä—ã–≤–æ–º)

3. –†–∞–∑–º–µ—Ç–∏—Ç—å –∂–∞–Ω—Ä—ã –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ

4. –†–∞—Å—Å—á–∏—Ç–∞—Ç—å Cohen's Kappa:
   ```python
   from sklearn.metrics import cohen_kappa_score

   kappa = cohen_kappa_score(annotator1, annotator2)
   print(f"Cohen's Kappa: {kappa:.3f}")
   ```

5. –ï—Å–ª–∏ Œ∫ < 0.7:
   - –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏—è
   - –£—Ç–æ—á–Ω–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–∞–∑–º–µ—Ç–∫–∏

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- Cohen's Kappa > 0.7
- –í–∞–ª–∏–¥–∞—Ü–∏—è ground truth

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [ ] –î–≤–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–∞
- [ ] Kappa —Ä–∞—Å—Å—á–∏—Ç–∞–Ω
- [ ] –ï—Å–ª–∏ –Ω–∏–∑–∫–∏–π ‚Äî —Ä–∞–∑–Ω–æ–≥–ª–∞—Å–∏—è –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã

---

### –ó–∞–¥–∞—á–∞ 4.4: –ù–∞–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏

**–°—Ä–æ–∫**: 2‚Äì3 –Ω–µ–¥–µ–ª–∏

**–®–∞–≥–∏**:

1. –°–æ–∑–¥–∞—Ç—å document: `paper/dialogue_2026_paper.tex` –∏–ª–∏ `.md`

2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ (—Å–æ–≥–ª–∞—Å–Ω–æ —à–∞–±–ª–æ–Ω—É Dialogue):

   **Abstract** (150‚Äì200 —Å–ª–æ–≤):
   - –ü—Ä–æ–±–ª–µ–º–∞
   - –ú–µ—Ç–æ–¥
   - –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
   - –í—ã–≤–æ–¥—ã

   **1. Introduction** (2‚Äì3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã):
   - –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –∂–∞–Ω—Ä–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
   - –ü—Ä–æ–±–ª–µ–º–∞: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏
   - RQs
   - –í–∫–ª–∞–¥

   **2. Related Work** (2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã):
   - –ñ–∞–Ω—Ä–æ–≤–∞—è —Ç–µ–æ—Ä–∏—è (Bhatia, Swales)
   - NLP –ø–æ–¥—Ö–æ–¥—ã –∫ –∂–∞–Ω—Ä–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
   - –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –≤ NLP

   **3. Data** (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞):
   - The Guardian corpus
   - –ñ–∞–Ω—Ä–æ–≤—ã–µ –º–µ—Ç–∫–∏
   - Inter-annotator agreement

   **4. Methods** (3‚Äì4 —Å—Ç—Ä–∞–Ω–∏—Ü—ã):
   - 4.1 TF‚ÄìIDF + LR
   - 4.2 Linguistic features + RF
   - 4.3 BERT fine-tuning
   - 4.4 Statistical validation

   **5. Results** (2‚Äì3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã):
   - 5.1 Overall performance (Table 1)
   - 5.2 Confusion matrices
   - 5.3 Error patterns

   **6. Discussion** (3‚Äì4 —Å—Ç—Ä–∞–Ω–∏—Ü—ã):
   - 6.1 Genre boundaries as gradient
   - 6.2 Error interpretation
   - 6.3 Attention analysis
   - 6.4 Limitations

   **7. Conclusion** (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞):
   - –í—ã–≤–æ–¥—ã
   - Future work

   **References**

3. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è Dialogue:
   - 8‚Äì12 —Å—Ç—Ä–∞–Ω–∏—Ü
   - LaTeX —à–∞–±–ª–æ–Ω —Å –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏
   - Deadline: –æ–±—ã—á–Ω–æ –∞–ø—Ä–µ–ª—å-–º–∞–π

**–†–µ–∑—É–ª—å—Ç–∞—Ç**:
- –ß–µ—Ä–Ω–æ–≤–∏–∫ —Å—Ç–∞—Ç—å–∏
- –í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ –∏ —Ç–∞–±–ª–∏—Ü—ã

**–ü—Ä–æ–≤–µ—Ä–∫–∞**:
- [ ] –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [ ] –í—Å–µ RQs –æ—Ç–≤–µ—á–µ–Ω—ã
- [ ] –õ–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü —Å–æ–±–ª—é–¥—ë–Ω
- [ ] References –æ—Ñ–æ—Ä–º–ª–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ

---

## –†–µ—Å—É—Ä—Å—ã –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã

### –ù–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –ü–û

```bash
# Python
python 3.9+

# NLP
pip install spacy transformers torch datasets scikit-learn

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
pip install matplotlib seaborn shap

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
pip install statsmodels scipy

# Jupyter
pip install jupyter ipywidgets

# spaCy –º–æ–¥–µ–ª—å
python -m spacy download en_core_web_sm
```

### –ñ–µ–ª–µ–∑–æ

- **GPU**: NVIDIA GPU —Å 8+ GB VRAM –∏–ª–∏ Apple Silicon MPS (–¥–ª—è BERT)
- **RAM**: 16+ GB
- **Disk**: 10+ GB —Å–≤–æ–±–æ–¥–Ω—ã—Ö

–ï—Å–ª–∏ –Ω–µ—Ç GPU:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Google Colab (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
- Kaggle Notebooks (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)

### –î–∞–Ω–Ω—ã–µ

- `cleaned_combined_guardian.csv` (—É–∂–µ –µ—Å—Ç—å)
- 50K —Ç–µ–∫—Å—Ç–æ–≤, 5 –∂–∞–Ω—Ä–æ–≤

---

## Milestones (–ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏)

| –ù–µ–¥–µ–ª—è | Milestone | –ß—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≥–æ—Ç–æ–≤–æ |
|--------|-----------|------------------------|
| 1‚Äì3 | Baseline models | TF‚ÄìIDF, Linguistic, BERT –æ–±—É—á–µ–Ω—ã |
| 4‚Äì6 | Error analysis | Confusion matrices, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ |
| 7‚Äì8 | Interpretation | Attention visualization |
| 9‚Äì10 | Validation | McNemar, bootstrap, inter-annotator |
| 11‚Äì12 | Paper draft | –ß–µ—Ä–Ω–æ–≤–∏–∫ —Å—Ç–∞—Ç—å–∏ –≥–æ—Ç–æ–≤ |

---

## –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–∞—Ü–∏—è

### –†–∏—Å–∫ 1: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏

**–ú–∏—Ç–∏–≥–∞—Ü–∏—è**:
- –ü—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è: BERT + –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ ‚Üí –º–∏–Ω–∏–º—É–º –¥–ª—è acceptance
- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: SHAP (–µ—Å–ª–∏ –Ω–µ —É—Å–ø–µ–≤–∞–µ–º)

### –†–∏—Å–∫ 2: –ù–∏–∑–∫–∞—è inter-annotator agreement

**–ú–∏—Ç–∏–≥–∞—Ü–∏—è**:
- –£—Ç–æ—á–Ω–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–æ –Ω–∞—á–∞–ª–∞
- –ï—Å–ª–∏ Œ∫ < 0.7 ‚Üí —á–µ—Å—Ç–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –≤ limitations

### –†–∏—Å–∫ 3: BERT –Ω–µ –≤–ª–µ–∑–∞–µ—Ç –≤ GPU

**–ú–∏—Ç–∏–≥–∞—Ü–∏—è**:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Colab
- Gradient accumulation
- DistilBERT (–º–µ–Ω—å—à–µ, —á—É—Ç—å —Ö—É–∂–µ)

### –†–∏—Å–∫ 4: –ù–µ—Ç —è–≤–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –æ—à–∏–±–∫–∞—Ö

**–ú–∏—Ç–∏–≥–∞—Ü–∏—è**:
- –†–∞—Å—à–∏—Ä–∏—Ç—å –≤—ã–±–æ—Ä–∫—É –æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
- –§–æ–∫—É—Å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ
- –ß–µ—Å—Ç–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å: "mixed results"

---

## Checklist –ø–µ—Ä–µ–¥ submission

### –ú–æ–¥–µ–ª–∏:
- [x] TF‚ÄìIDF –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω
- [x] Linguistic features –∏–∑–≤–ª–µ—á–µ–Ω—ã
- [x] BERT –æ–±—É—á–µ–Ω

### –ê–Ω–∞–ª–∏–∑:
- [x] Confusion matrices –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
- [x] 30‚Äì50 –æ—à–∏–±–æ—á–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–æ–±—Ä–∞–Ω–æ
- [x] –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–≤–µ–¥—ë–Ω
- [x] Attention visualization

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
- [ ] McNemar's test
- [ ] Bootstrap CIs
- [ ] Inter-annotator agreement

### –°—Ç–∞—Ç—å—è:
- [ ] Abstract –Ω–∞–ø–∏—Å–∞–Ω
- [ ] Introduction —Å RQs
- [ ] Methods –¥–µ—Ç–∞–ª—å–Ω–æ –æ–ø–∏—Å–∞–Ω—ã
- [ ] Results —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏/–≥—Ä–∞—Ñ–∏–∫–∞–º–∏
- [ ] Discussion —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
- [ ] Conclusion
- [ ] References
- [ ] –õ–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü —Å–æ–±–ª—é–¥—ë–Ω
- [ ] LaTeX/LaTeX —à–∞–±–ª–æ–Ω —Å–æ–±–ª—é–¥—ë–Ω

---

## –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏

### –ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è Dialogue:
- https://dialogue-conf.org/
- –®–∞–±–ª–æ–Ω—ã: https://dialogue-conf.org/submission.html

### –ñ–∞–Ω—Ä–æ–≤–∞—è —Ç–µ–æ—Ä–∏—è:
- Bhatia, V. K. (1993). Analysing Genre: Language Use in Professional Settings
- Swales, J. M. (2004). Research Genres: Explorations and Applications

### NLP –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- HuggingFace: https://huggingface.co/docs
- spaCy: https://spacy.io/usage
- SHAP: https://shap.readthedocs.io/

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
- McNemar test: https://en.wikipedia.org/wiki/McNemar%27s_test
- Bootstrap: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–¥–µ–∏ (–µ—Å–ª–∏ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –≤—Ä–µ–º—è)

1. **Cross-lingual**: —Ç–æ –∂–µ —Å–∞–º–æ–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ (–õ–µ–Ω—Ç–∞.—Ä—É, –ì–∞–∑–µ—Ç–∞.—Ä—É)
2. **Diachronic**: —Å—Ä–∞–≤–Ω–∏—Ç—å 2015‚Äì2017 vs 2023‚Äì2025
3. **Probing classifiers**: –∫–∞–∫–∏–µ —Å–ª–æ–∏ BERT –∫–æ–¥–∏—Ä—É—é—Ç –∂–∞–Ω—Ä–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é?
4. **Human evaluation**: –ø–æ–∫–∞–∑–∞—Ç—å –ª—é–¥—è–º –æ—à–∏–±–æ—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã, —Å–ø—Ä–æ—Å–∏—Ç—å ‚Äî –∫–∞–∫–æ–π –∂–∞–Ω—Ä?

---

**–£–¥–∞—á–∏ —Å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ–º! üöÄ**
