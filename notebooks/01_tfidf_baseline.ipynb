{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1: TFâ€“IDF + Logistic Regression Baseline\n",
    "\n",
    "**Goal**: Establish a baseline for genre classification using TFâ€“IDF features and Logistic Regression\n",
    "\n",
    "**Expected performance**: Accuracy ~70-75%, Macro F1 ~0.68-0.73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/cleaned_combined_guardian.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique categories\n",
    "print(\"Unique categories:\")\n",
    "print(df['category'].value_counts())\n",
    "print(f\"\\nTotal unique categories: {df['category'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['category'].value_counts().plot(kind='bar')\n",
    "plt.title('Category Distribution')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check Missing Values and Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df[['category', 'cleaned_text']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check text length statistics\n",
    "df['text_length'] = df['cleaned_text'].str.len()\n",
    "print(\"Text length statistics:\")\n",
    "print(df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['text_length'], bins=50, edgecolor='black')\n",
    "plt.title('Text Length Distribution')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df['text_length'])\n",
    "plt.title('Text Length Boxplot')\n",
    "plt.ylabel('Text Length (characters)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "df = df.dropna(subset=['category', 'cleaned_text'])\n",
    "print(f\"After removing NaN: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove very short texts (< 50 characters)\n",
    "df = df[df['cleaned_text'].str.len() > 50]\n",
    "print(f\"After removing short texts: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove very long texts (> 20000 characters) - potential outliers\n",
    "df = df[df['cleaned_text'].str.len() < 20000]\n",
    "print(f\"After removing very long texts: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final category distribution\n",
    "print(\"Final category distribution:\")\n",
    "print(df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract texts and labels\n",
    "texts = df['cleaned_text'].tolist()\n",
    "labels = df['category'].tolist()\n",
    "\n",
    "print(f\"Total samples: {len(texts)}\")\n",
    "print(f\"Labels: {set(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Split Data (Train/Val/Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 80% train, 20% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% val, 50% test from temp (10% val, 10% test from total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTotal: {len(X_train) + len(X_val) + len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution in splits\n",
    "print(\"\\nTraining label distribution:\")\n",
    "print(Counter(y_train))\n",
    "print(\"\\nValidation label distribution:\")\n",
    "print(Counter(y_val))\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits for reuse\n",
    "splits = {\n",
    "    'X_train': X_train,\n",
    "    'X_val': X_val,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "joblib.dump(splits, '../results/data_splits.pkl')\n",
    "print(\"Data splits saved to ../results/data_splits.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. TFâ€“IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,      # Top 10k words\n",
    "    ngram_range=(1, 2),      # Unigrams + bigrams\n",
    "    min_df=5,                # Min 5 documents\n",
    "    max_df=0.8,              # Max 80% documents\n",
    "    stop_words='english'     # Remove stop words\n",
    ")\n",
    "\n",
    "print(\"TFâ€“IDF vectorizer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training data and transform all sets\n",
    "print(\"Fitting vectorizer on training data...\")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Validation set shape: {X_val_tfidf.shape}\")\n",
    "print(f\"Test set shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Grid Search for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization strength\n",
    "    'penalty': ['l2'],        # L2 regularization\n",
    "    'solver': ['lbfgs']       # Solver for multinomial\n",
    "}\n",
    "\n",
    "# Initialize logistic regression\n",
    "lr_base = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    multi_class='multinomial'\n",
    ")\n",
    "\n",
    "print(\"Starting Grid Search...\")\n",
    "print(f\"Parameter grid: {param_grid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with 5-fold CV\n",
    "grid_search = GridSearchCV(\n",
    "    lr_base,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters and score\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_model.predict(X_train_tfidf)\n",
    "y_val_pred = best_model.predict(X_val_tfidf)\n",
    "y_test_pred = best_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Predictions completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set performance\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "train_f1_macro = f1_score(y_train, y_train_pred, average='macro')\n",
    "\n",
    "print(\"=== Training Set ===\")\n",
    "print(f\"Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Macro F1: {train_f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set performance\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "val_f1_macro = f1_score(y_val, y_val_pred, average='macro')\n",
    "\n",
    "print(\"=== Validation Set ===\")\n",
    "print(f\"Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Macro F1: {val_f1_macro:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set performance\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "print(\"=== Test Set ===\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Macro F1: {test_f1_macro:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Get class labels\n",
    "class_labels = best_model.classes_\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/tfidf_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion matrix saved to ../results/tfidf_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Extract Lexical Markers for Each Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Total features: {len(feature_names)}\")\n",
    "print(f\"\\nClass labels: {class_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top words for each genre\n",
    "lexical_markers = {}\n",
    "\n",
    "for i, genre in enumerate(class_labels):\n",
    "    # Get coefficients for this class\n",
    "    coef = best_model.coef_[i]\n",
    "    \n",
    "    # Get top 50 words with highest weights\n",
    "    top_indices = np.argsort(coef)[-50:][::-1]\n",
    "    \n",
    "    # Store word-weight pairs\n",
    "    lexical_markers[genre] = [\n",
    "        (feature_names[idx], coef[idx])\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "\n",
    "print(\"Lexical markers extracted for all genres!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 20 words for each genre\n",
    "for genre, markers in lexical_markers.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENRE: {genre.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for word, weight in markers[:20]:\n",
    "        print(f\"{word:25s} {weight:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorizer\n",
    "joblib.dump(vectorizer, '../models/tfidf_vectorizer.pkl')\n",
    "print(\"âœ“ Vectorizer saved to ../models/tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(best_model, '../models/tfidf_lr.pkl')\n",
    "print(\"âœ“ Model saved to ../models/tfidf_lr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save confusion matrix\n",
    "joblib.dump(cm, '../results/tfidf_confusion_matrix.npy')\n",
    "print(\"âœ“ Confusion matrix saved to ../results/tfidf_confusion_matrix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lexical markers\n",
    "joblib.dump(lexical_markers, '../results/tfidf_lexical_markers.pkl')\n",
    "print(\"âœ“ Lexical markers saved to ../results/tfidf_lexical_markers.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics = {\n",
    "    'model': 'TFâ€“IDF + Logistic Regression',\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'train_accuracy': float(train_acc),\n",
    "    'train_f1_macro': float(train_f1_macro),\n",
    "    'val_accuracy': float(val_acc),\n",
    "    'val_f1_macro': float(val_f1_macro),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_f1_macro': float(test_f1_macro),\n",
    "    'cv_score': float(grid_search.best_score_)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../results/tfidf_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Metrics saved to ../results/tfidf_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TFâ€“IDF + LOGISTIC REGRESSION - FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"\\nCV Score (F1-macro): {grid_search.best_score_:.4f}\")\n",
    "print(f\"\\n--- Test Set Performance ---\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Macro F1: {test_f1_macro:.4f}\")\n",
    "print(f\"\\n--- Expected vs Actual ---\")\n",
    "print(f\"Expected Accuracy: ~70-75%\")\n",
    "print(f\"Actual Accuracy: {test_acc:.2%}\")\n",
    "print(f\"\\nExpected Macro F1: ~0.68-0.73\")\n",
    "print(f\"Actual Macro F1: {test_f1_macro:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Summary\n",
    "\n",
    "### âœ… Completed:\n",
    "1. Loaded and explored data (50K texts from The Guardian)\n",
    "2. Cleaned data (removed NaN, very short/long texts)\n",
    "3. Split data into train/val/test (80/10/10)\n",
    "4. Created TFâ€“IDF vectors (10k features, unigrams + bigrams)\n",
    "5. Performed Grid Search for Logistic Regression\n",
    "6. Extracted lexical markers for each genre (top 50 words)\n",
    "7. Saved model, vectorizer, and results\n",
    "\n",
    "### ðŸ“Š Results:\n",
    "- Test Accuracy: **{:.2%}\".format(test_acc))\n",
    "- Test Macro F1: **{:.4f}\".format(test_f1_macro))\n",
    "- Confusion matrix saved\n",
    "- Lexical markers for each genre extracted\n",
    "\n",
    "### ðŸŽ¯ Next Steps:\n",
    "1. Analyze confusion matrix for error patterns\n",
    "2. Compare with linguistic features model\n",
    "3. Compare with BERT/RoBERTa\n",
    "\n",
    "### ðŸ“ Files Created:\n",
    "- `../models/tfidf_vectorizer.pkl` - TFâ€“IDF vectorizer\n",
    "- `../models/tfidf_lr.pkl` - Trained Logistic Regression model\n",
    "- `../results/tfidf_metrics.json` - Performance metrics\n",
    "- `../results/tfidf_confusion_matrix.npy` - Confusion matrix\n",
    "- `../results/tfidf_confusion_matrix.png` - Confusion matrix visualization\n",
    "- `../results/tfidf_lexical_markers.pkl` - Top words for each genre\n",
    "- `../results/data_splits.pkl` - Train/val/test splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
