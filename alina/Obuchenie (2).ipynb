{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d1d713-c936-4de1-8c8b-1c1393106dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658adf6f-b6e1-4494-9aad-7635ffba642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Проверка GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6170ea4f-97ac-4f1f-a587-aa31c560f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Загрузка данных\n",
    "df = pd.read_csv(\"cleaned_combined_guardian.csv\")\n",
    "texts = df[\"cleaned_text\"].tolist()\n",
    "categories = df[\"category\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2cc926-eee6-4087-a3f3-b149812d6e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 invalid texts:\n",
      "Empty DataFrame\n",
      "Columns: [cleaned_text, category]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=[\"cleaned_text\", \"category\"])\n",
    "\n",
    "# Конвертация и очистка текстов\n",
    "df[\"cleaned_text\"] = (\n",
    "    df[\"cleaned_text\"]\n",
    "    .astype(str)\n",
    "    .replace({\"nan\": \"\", \"None\": \"\", \"null\": \"\"})  # Очистка скрытых NaN\n",
    ")\n",
    "\n",
    "# Проверка проблемных значений\n",
    "nan_mask = df[\"cleaned_text\"].str.strip().isin([\"\", \"nan\", \"None\", \"null\"])\n",
    "print(f\"Found {nan_mask.sum()} invalid texts:\")\n",
    "print(df.loc[nan_mask, [\"cleaned_text\", \"category\"]].head())\n",
    "\n",
    "# Удаление строк с пустыми текстами после конвертации\n",
    "df = df[~nan_mask]\n",
    "\n",
    "# Дополнительная проверка типов\n",
    "assert df[\"cleaned_text\"].apply(lambda x: isinstance(x, str)).all()\n",
    "assert df[\"category\"].apply(lambda x: isinstance(x, str)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c22ccb8-cc77-4ed6-ba55-ae03752fea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для модели\n",
    "texts = df[\"cleaned_text\"].tolist()\n",
    "categories = df[\"category\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed007a8d-715c-4058-a4b0-0cc1a3707693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование типов\n",
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].astype(str)\n",
    "df[\"category\"] = df[\"category\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35a0efb7-686c-4f2b-a625-1fffcfc40338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный размер датасета: 49993\n",
      "Размер после очистки: 49993\n"
     ]
    }
   ],
   "source": [
    "# Точечное удаление 7 проблемных строк\n",
    "print(f\"Исходный размер датасета: {len(df)}\")\n",
    "df = df.dropna(subset=[\"cleaned_text\"])  # Удаляем только 7 строк с NaN в текстах\n",
    "print(f\"Размер после очистки: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78f5cef4-bf36-45fe-b39c-4c1e80c18a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Проверка очищенных данных:\n",
      "NaN в cleaned_text: 0\n",
      "Пример текста: hsbc has sounded the alarm about the impact of higher trade tariffs on economic growth unemployment ...\n",
      "Уникальные категории: ['News' 'Analytical' 'Feature' 'Editorial' 'Review']\n"
     ]
    }
   ],
   "source": [
    "# Проверка результатов\n",
    "print(\"\\nПроверка очищенных данных:\")\n",
    "print(f\"NaN в cleaned_text: {df['cleaned_text'].isna().sum()}\")\n",
    "print(f\"Пример текста: {df['cleaned_text'].iloc[0][:100]}...\")\n",
    "print(f\"Уникальные категории: {df['category'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359da07e-fbb6-4610-bd9b-40938211d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодирование категорий\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(categories)\n",
    "\n",
    "# Разделение данных (добавим стратификацию)\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, \n",
    "    temp_labels, \n",
    "    test_size=0.5, \n",
    "    stratify=temp_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Дополнительная проверка перед созданием Dataset\n",
    "def validate_texts(text_list):\n",
    "    for i, text in enumerate(text_list):\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(f\"Non-string text at index {i}: {type(text)}\")\n",
    "        if len(text.strip()) == 0:\n",
    "            raise ValueError(f\"Empty text at index {i}\")\n",
    "\n",
    "validate_texts(train_texts)\n",
    "validate_texts(val_texts)\n",
    "validate_texts(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91493054-d166-4c34-8810-7d933b5436e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Создание Dataset\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e77ca91-5607-4115-8319-0800f36da08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca1dc9ca3d04f73a8a4e109b44e0c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39994 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7323b2e94643bda75a55047b26f164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bfa107ffd5411d800e10229309a308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. Правильная токенизация с удалением исходных текстов\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": examples[\"label\"]\n",
    "    }\n",
    "\n",
    "# Применяем токенизацию и удаляем исходные тексты\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_test = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25cee8eb-ffc2-4360-ba96-c5232804f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Настройка DataLoader\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    "    pad_to_multiple_of=8,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d0e0e6f-5409-4e7a-9388-525a2f60e75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 7. Модель\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=5\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "485f20bb-c861-4bd7-bc8e-4d10801291b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Оптимизатор и планировщик\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a3915f5-6dfe-459d-b4f0-c3ca24b70f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Пример батча:\n",
      "{'input_ids': torch.Size([16, 256]), 'attention_mask': torch.Size([16, 256]), 'labels': torch.Size([16])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlad-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_dataloader))\n",
    "print(\"\\nПример батча:\")\n",
    "print({k: v.shape for k, v in sample_batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af27922-e1f3-4831-a7d1-fc6bcb5d4218",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "    tokenized_val,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_train,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_val,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_test,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "898f7a18-ecc7-4cc6-bc2b-e1a7752efc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf22dac83a43428aa156fe0c371ec13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6d3d7b53d74dc384c8651f85045afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d884dea373314b79b7ce86987eb8b8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb617e0f209f44a198f2453c08c30945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba25847c5c5a49adae1e15d525ab356d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d57d3d775cd452e8dfe682e101ed983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8811\n"
     ]
    }
   ],
   "source": [
    "# 9. Обучение\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Тренировка\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_accuracy = []\n",
    "    for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        accuracy = (predictions == batch[\"labels\"]).float().mean()\n",
    "        val_accuracy.append(accuracy.item())\n",
    "    \n",
    "    avg_val_accuracy = np.mean(val_accuracy)\n",
    "    print(f\"Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
    "\n",
    "    # Сохранение лучшей модели\n",
    "    if avg_val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = avg_val_accuracy\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63c97e82-a2b9-4259-8343-9285cf4ef76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9dea09366c434e8efb05d9a91088ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8762\n"
     ]
    }
   ],
   "source": [
    "# 10. Тестирование\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "test_accuracy = []\n",
    "for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    accuracy = (predictions == batch[\"labels\"]).float().mean()\n",
    "    test_accuracy.append(accuracy.item())\n",
    "\n",
    "print(f\"Test Accuracy: {np.mean(test_accuracy):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb2ef2ea-4589-45c1-87af-2a293ff4ff73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_category_classifier\\\\tokenizer_config.json',\n",
       " './bert_category_classifier\\\\special_tokens_map.json',\n",
       " './bert_category_classifier\\\\vocab.txt',\n",
       " './bert_category_classifier\\\\added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11. Сохранение модели и токенизатора\n",
    "model.save_pretrained(\"./bert_category_classifier\")\n",
    "tokenizer.save_pretrained(\"./bert_category_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5031d10a-49e0-4962-8290-1068fffedfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(categories)\n",
    "\n",
    "# Сохранение энкодера\n",
    "import joblib\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b09c568-6ad0-4eb1-a55c-ae7ac992f51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
