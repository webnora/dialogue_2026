# Текст выступления: Что векторные представления рассказывают о публицистическом стиле

**Конференция Dialogue 2026**
**Январь 2026**

---

## Слайд 1: Титульный

**Доброе утро, коллеги. Сегодня я представлю наше исследование "Что векторные представления рассказывают о публицистическом стиле: учимся на ошибках".**

Это совместный проект о классификации жанров журналистских текстов, но с неожиданным поворотом: вместо того чтобы просто гнаться за высокой точностью, мы интересуемся тем, что ошибки классификации могут рассказать нам о природе жанровых границ.

---

## Слайд 2: Исследовательский вопрос

**Позвольте начать с фундаментального вопроса: Почему классификация жанров важна?**

Когда мы читаем газету, мы интуитивно понимаем — читаем ли мы новостную заметку, opinion-статью или очерк. Но для компьютеров это гораздо сложнее. Почему? Потому что жанры не всегда чётко определены — они пересекаются, накладываются друг на друга, и реальные статьи часто смешивают элементы нескольких жанров.

**Наш исследовательский вопрос: Что ошибки классификации рассказывают о жанровых границах?**

У нас есть четыре ключевые гипотезы:

**H1: Контекстуальные модели вроде BERT должны превосходить лексические модели вроде TF-IDF.** Это общепринятое мнение — глубокое понимание должно помогать в классификации жанров.

**H2: Жанровые границы градиентны, а не дискретны.** Вместо чётких границ жанры существуют на континууме.

**H3: Лексический выбор — основной маркер жанра.** Другими словами, *слова важнее всего* для определения жанра.

**H4: Синтаксические признаки менее информативны, чем лексические.** Такие вещи как структура предложения и грамматические паттерны вторичны по отношению к выбору слов.

---

## Слайд 3: Методология — три уровня репрезентации

**Итак, как мы тестируем эти гипотезы? Мы сравниваем три разных подхода к представлению текста, каждый на другом уровне лингвистического анализа.**

**Первый уровень: Лексический.** Это самый простой — мы просто смотрим на слова. Конкретно, мы используем TF-IDF (Term Frequency-Inverse Document Frequency) с логистической регрессией. Что делает TF-IDF? Он считает, как часто каждое слово встречается в тексте, но даёт больший вес редким словам, которые являются отличительными. Например, "said" и "yesterday" могут часто встречаться в новостях, а "I" и "my" могут указывать на очерк. Ключевая идея здесь в том, что мы полностью игнорируем порядок слов и контекст — мы просто рассматриваем каждый текст как "мешок слов".

**Второй уровень: Дискурсивно-грамматический.** Вместо того чтобы смотреть, какие слова встречаются, мы извлекаем лингвистические признаки вроде: Какой длины предложения? Сколько местоимений первого лица ("I", "we") используется? Каково соотношение глаголов репортажа вроде "said" и "claimed"? Эти признаки захватывают *структуру* и *стиль* письма, а не конкретный словарный запас. Мы обучаем случайный лес (Random Forest) на этих десяти рукотворных признаках.

**Третий уровень: Контекстуально-семантический.** Это BERT — глубокая нейронная сеть с 110 миллионами параметров, которая была предобучена на миллиардах слов, а затем дообучена на нашей задаче классификации жанров. В отличие от TF-IDF, BERT понимает контекст. Он знает, что "bank" означает разные вещи в "river bank" и "bank account". Он захватывает глубокие семантические отношения и долгосрочные зависимости в тексте.

**Красота этого подхода в том, что каждый уровень представляет другую лингвистическую теорию о том, что важно для жанра.**

---

## Слайд 4: Датасет

**Теперь давайте поговорим о наших данных.**

Мы используем статьи из газеты The Guardian, собранные через их API. Почему The Guardian? У них хорошо организованная система тегирования жанров, что даёт нам надёжные метки.

У нас пять жанров, по 10,000 статей в каждом:

**News** — простые фактические репортажи, перевёрнутая пирамида, только факты. Думайте: "Премьер-министр вчера объявил новую политику."

**Analytical** статьи — управляемые данными. Они включают диаграммы, статистику, глубокий анализ. Они всё ещё фактические, но более интерпретативные, чем новости.

**Feature** статьи — истории о людях. Они используют нарративные техники, развитие персонажей, сторителлинг. Думайте об этом как о длинной форме журналистики.

**Editorial** — мнение, официальная позиция газеты по вопросам. Это убедительно, аргументированно, субъективно.

**Review** — культурная критика — книги, фильмы, театр, рестораны.

После очистки — удаления HTML-тегов, URL, очень коротких или очень длинных статей — у нас около 49,000 статей, разделённых 80-10-10 на обучающую-валидационную-тестовую выборки.

---

## Слайд 5: Фаза 1 — Сравнение моделей

**Давайте посмотрим на результаты.**

**BERT достигает 87.64% точности.** Это лучшая производительность. Это глубокая контекстуальная модель, которая понимает семантику.

**TF-IDF с логистической регрессией достигает 86.58%.** Это всего на 1.06% хуже, чем BERT. Подумайте, что это значит: простая модель подсчёта слов почти так же хороша, как современная нейронная сеть с 110 миллионами параметров.

**Лингвистическая модель признаков достигает 65% точности.** Это dramatically хуже — более чем на 20% ниже лексических моделей.

**Эти числа говорят нам что-то важное.**

---

## Слайд 6: Ключевой вывод 1 — Лексическая первичность

**Вот наш главный вывод: Лексическая первичность.**

Разрыв между BERT и TF-IDF составляет всего 1.06%. Позвольте мне это perspective.

Если бы контекст и глубокое семантическое понимание были критически важны для классификации жанров, мы бы ожидали, что BERT значительно превзойдёт TF-IDF — может быть, на 5-10% или более. Но это не то, что мы видим.

**Что это значит — то, что примерно 99% жанрового сигнала захватывается только выбором слов.** Фактические слова, которые вы используете, важнее, чем как они расположены, их грамматический контекст или их глубокое семантическое значение.

Это бросает вызов общепринятому предположению в NLP, что нам всегда нужны сложные контекстуальные эмбеддинги. Для практических приложений —比如 автоматическое тегирование статей по жанру — TF-IDF может быть вполне адекватным. Он быстрее, проще и требует гораздо меньше вычислительной мощности.

**Теоретическое имплицитное значение в том, что жанр — это прежде всего лексический феномен.** Когда мы пишем в определённом жанре, мы принимаем характерный словарный запас. Авторы новостей используют определённые слова; авторы очерков используют другие; критики используют третьи. И этот словарный запас настолько отличительный, что один только он может предсказать жанр с почти 90% точностью.

---

## Слайд 7: Ключевой вывод 2 — Синтаксические признаки нед Performant

**Наш второй вывод: Почему лингвистические признаки так плохо работают?**

Лингвистическая модель достигает только 65% точности — едва лучше случайного угадывания (которое было бы 20% для 5 жанров).

Мы можем увидеть, что модель считает важным, посмотрев на важность признаков:

**Самый важный признак — это соотношение глаголов репортажа** — слова вроде "said", "claimed", "announced". Это имеет смысл: в новостях полно цитат и репортируемой речи.

**Второй — type-token ratio** — мера разнообразия словарного запаса. Аналитические статьи склонны использовать более разнообразный словарь.

**Третий — местоимения первого лица** — "I", "my", "me". Features и editorials используют их чаще, чем новости.

Но здесь проблема: модель severely overfitting. На обучающих данных она достигает 87% точности, но на тестовых данных падает до 65%. Этот разрыв в 22 процентных пункта указывает на то, что эти признаки не являются надёжными предикторами.

**Почему это происходит? Я думаю, потому что жанр просто не определяется этими грубыми структурными признаками.** Вы можете написать новостную историю с длинными предложениями и кучей местоимений первого лица — это была бы необычная новость, но это всё ещё была бы новость из-за *того, о чём* она и *какие слова* она использует. Лингвистические признаки слишком поверхностны, чтобы захватить суть жанра.

---

## Слайд 7.5: Статистическая валидация — Тест McNemar'а

**Теперь я хочу обратиться к критическому вопросу: Значимы ли эти различия статистически, или они могут быть из-за случайного шанса?**

Мы используем **тест McNemar'а** для сравнения моделей. Позвольте мне объяснить, что делает этот тест.

Когда мы оцениваем две модели на одном тестовом наборе, мы можем создать таблицу сопряжённости. Для каждого тестового примера есть четыре возможности: обе модели правильны; обе модели неправильны; модель A права и модель B неправильна; или модель B права и модель A неправильна.

Тест McNemar'а фокусируется на случаях, когда модели *не согласны* — где одна права, а другая неправильна. Он спрашивает: Сбалансировано ли это несогласие, или одна модель последовательно превосходит другую?

**Вот наши результаты:**

Когда мы сравниваем BERT и TF-IDF, мы получаем chi-squared statistic 156.01 с p-value менее 0.001. Это значит, что вероятность увидеть эту разницу случайно — менее одного из тысячи. Это **высоко статистически значимо**.

Аналогично, BERT против Linguistic даёт chi-squared 258.34, а TF-IDF против Linguistic даёт 31.07. Оба значимы при p < 0.001.

**Что это значит на простом языке?** Это значит, что эти различия в производительности реальны и робастны. Мы можем уверенно сказать, что BERT лучше, чем TF-IDF, и оба намного лучше, чем лингвистическая модель признаков. Это не флуктуации — это настоящие различия.

---

## Слайд 7.6: Bootstrap доверительные интервалы

**Но точность — это просто одно число. Насколько мы можем быть уверены в этой оценке?**

Мы используем **bootstrap доверительные интервалы** для количественной оценки неопределённости. Позвольте мне объяснить метод бутстрэппинга.

Представьте, что наш тестовый набор имеет 5,000 статей. Bootstrap создаёт новые "фейковые" тестовые наборы, случайно сэмплируя из этих 5,000 статей *с заменой*. Это значит, что некоторые статьи могут появиться несколько раз, а другие могут не появиться вообще. Мы делаем это 1,000 раз, каждый раз вычисляя точность. Это даёт нам распределение из 1,000 значений точности.

95% доверительный интервал — это средние 95% этого распределения — от 2.5-го персентиля до 97.5-го персентиля. Он говорит нам: "Мы на 95% уверены, что истинная точность лежит в этом диапазоне."

**Вот наши результаты bootstrap:**

BERT достигает 92.73% точности с 95% доверительным интервалом [92.02%, 93.42%]. Так что мы можем быть довольно уверены, что истинная точность BERT между 92% и 93.4%.

TF-IDF имеет 86.40% точности с CI [85.50%, 87.22%].

Linguistic имеет 82.85% точности с CI [81.78%, 83.94%].

**Ключевая идея в том, что эти интервалы не пересекаются.** Нижняя граница BERT (92.02%) выше, чем верхняя граница TF-IDF (87.22%). Это другой способ подтверждения того, что различия статистически значимы. Есть чёткий разрыв между моделями.

---

## Слайд 7.7: Согласие моделей — Cohen's Kappa

**Вот другая перспектива: Согласны ли эти модели друг с другом?**

Мы измеряем **межмодельное согласие**, используя Cohen's Kappa. Позвольте мне объяснить, почему это важно.

Если две модели соглашаются на 90% статей, это может звучать хорошо. Но что, если они оба просто случайно угадывают? С 5 жанрами случайное угадывание дало бы 20% согласия по случаю alone. Так что нам нужно скорректировать на это случайное согласие.

Cohen's Kappa делает именно это. Он измеряет согласие *превосходящее случайность*.

**Наши результаты:**

TF-IDF и BERT имеют Kappa 0.827.
Linguistic и BERT имеют Kappa 0.752.
TF-IDF и Linguistic имеют Kappa 0.728.

Согласно шкале Landis-Koch, значения Kappa между 0.61 и 0.80 указывают на "существенное согласие".

**Что это нам говорит?** Это говорит нам, что все три модели захватывают *один и тот же базовый сигнал*. Они не случайно правильны на разных статьях — они склонны соглашаться на том, какой жанр.

Это важно, потому что это значит, что есть последовательное понятие жанра, которое все модели могут обнаружить, даже если они используют разные подходы. Несогласия между моделей — это интересные случаи — пограничные случаи, где жанр действительно неоднозначен.

---

## Слайд 8: Фаза 2 — Анализ ошибок

**Теперь давайте углубимся в Фазу 2: анализ того, где модели совершают ошибки.**

Мы проанализировали 5,000 тестовых образцов. Вот что мы нашли:

**Во-первых, все три модели соглашаются на 73.4% статей.** Когда они все соглашаются, они правильны 98.4% времени. Это подтверждает, что когда есть чёткий жанровый сигнал, все модели могут его обнаружить.

**Но 27.8% статей — то есть 1,390 из 5,000 — это "гибридные" статьи.** Это где по крайней мере одна модель не соглашается с другими. Это пограничные случаи.

И есть 105 статей — всего 2.1% — где *все три* модели неправильны. Это действительно неоднозначные случаи, где даже лучшая модель не может понять жанр.

**Это сильное свидетельство градиентных жанровых границ.** Если бы жанры были дискретными категориями с чёткими границами, мы бы ожидали, что модели в основном соглашаются и в основном правильны. Вместо этого мы видим существенное несогласие, и это несогласие коррелирует с настоящей неоднозначностью.

---

## Слайд 9: Feature как "хаб-жанр"

**Одна из самых интересных находок в том, что Feature действует как "хаб-жанр".**

Что я имею в виду под "хабом"? Feature притягивает путаницу от всех других жанров.

Когда BERT неправильно классифицирует аналитическую статью, она с наибольшей вероятностью будет помечена как Feature — это случается 11.5% времени.

Когда BERT неправильно классифицирует новостную статью, она часто помечается как Feature — 5.4% времени.

Когда BERT неправильно классифицирует обзор, она иногда помечается как Feature — 4.2% времени.

**Почему это происходит?** Я думаю, Feature занимает среднюю позицию в жанровом пространстве. Она сочетает нарративные техники (like личные истории) с информативным контентом (like факты и контекст). Это сторителлинг с целью.

Аналитические статьи могут дрейфовать в территорию Feature, когда используют кейсы и нарративы. Новостные статьи могут стать feature-like, когда они рассказывают человеческую историю за новостями. Обзоры могут напоминать Features, когда включают фоновый контекст и сторителлинг.

**Feature — это коннектор между жанрами.** Это то место, где жанры перекрываются и смешиваются.

---

## Слайд 10: Градиентные границы

**Позвольте мне резюмировать свидетельства градиентных жанровых границ.**

У нас есть две категории проблемных статей:

**Категория 1: Все три модели неправильны.** Это 105 статей — 2.1% нашего тестового набора. Это действительно неоднозначно. Может быть, они были неправильно помечены в первую очередь, или может они смешивают жанры так тщательно, что никакой чёткой классификации не существует. В этих случаях сама ground truth метка questionablena.

**Категория 2: Модели не соглашаются.** Это 1,390 статей — 27.8%. Здесь некоторые модели правильны, некоторые неправильны. Несогласие не случайное — оно систематическое. Оно раскрывает структурные сходства между жанрами.

Например, News и Feature путают друг с другом, потому что обе могут рассказывать фактические истории. Analytical и Editorial путают, потому что opinionated analysis переходит в opinion. Feature и Review путают, потому что культурные Features могут напоминать Reviews.

**Это именно то, что мы ожидали бы, если жанры градиентны, а не дискретны.** Нет жёстких границ, только зоны перекрытия, где один жанр постепенно переходит в другой.

---

## Слайд 11: Фаза 3 — Интерпретация BERT через Attention

**Теперь переходим к Фазе 3: открывая чёрный ящик.**

BERT часто критикуют как чёрный ящик — он работает хорошо, но мы не знаем почему. Мы решаем это, анализируя **механизмы внимания** BERT.

Позвольте мне объяснить, что такое внимание простыми словами.

Когда вы читаете предложение, вы не обрабатываете все слова одинаково. Некоторые слова более важны для понимания смысла. BERT делает то же самое — он уделяет больше "внимания" некоторым словам, чем другим.

У BERT 12 слоёв, и в каждом слое 12 "attention heads". Каждая голова внимания учится фокусироваться на разных отношениях между словами. Например, одна голова может научиться связывать местоимения с их антецедентами (связывать "it" обратно с "the cat"). Другая может научиться связывать прилагательные с существительными.

Когда мы обучаем BERT для классификации жанров, эти головы внимания учатся фокусироваться на словах, которые наиболее диагностические для жанра. **Исследуя, какие слова получают больше внимания, мы можем увидеть, что BERT считает важным для определения жанра.**

Мы извлекли веса внимания из последних 6 слоёв BERT для 50 текстов (10 из каждого жанра). Затем мы проанализировали, какие токены получили самое высокое внимание.

---

## Слайд 12: Жанрово-специфические маркеры

**Вот что мы нашли: У каждого жанра есть характерные high-attention токены.**

**Для аналитических статей** BERT уделяет больше всего внимания политическим именам: "boris", "johnson", "downing" (как в Downing Street). Это имеет смысл — аналитические статьи в The Guardian часто фокусируются на политическом анализе, и конкретные политики упоминаются повторно.

**Для редакционных статей** BERT уделяет внимание "mr", "nhs" и служебным словам вроде "of". Редакционные статьи часто обсуждают институты и политики формальным тоном, используя титулы и институциональный язык.

**Для очерков** BERT фокусируется на "i", "my", "said". Это раскрывает нарративное качество — местоимения первого лица указывают на личный сторителлинг, а "said" указывает на диалоги и цитаты, которые распространены в очерках, которые интервьюируют людей.

**Для новостей** BERT уделяет внимание "trump", "masters", "after". Новости управляются событиями, так что они фокусируются на ключевых actory (как Trump) и временных маркерах вроде "after", которые помещают события в последовательность.

**Для обзоров** BERT выделяет "staging", "debut", "opera". Это domain-specific культурные термины — словарный запас искусств criticism.

**Это показывает, что BERT учится фокусироваться на жанро-диагностическом словаре.** Это не магия — он идентифицирует слова, которые характеризуют каждый жанр, так же как TF-IDF делает, но с добавленной способностью понимать контекст и отношения.

---

## Слайд 13: Универсальные маркеры

**Но в истории больше.**

В дополнение к жанрово-специфическим маркерам мы нашли **универсальные маркеры** — токены, которые получают высокое внимание *во всех жанрах*.

Это включает служебные слова вроде "we", "related" и "sp" (специальный токен).

**Это критически важно: нет уникального словаря для любого жанра.** Все жанры используют "we", все жанры используют определённые служебные слова. Паттерны внимания существенно перекрываются.

**Это объясняет, почему жанровые границы градиентны.** Если бы у жанров были совершенно различные словари — если бы News никогда не использовал слова, которые использует Feature, — тогда классификация была бы почти идеальной, и границы были бы острыми. Но это не то, что мы видим.

Вместо этого жанры делят большую часть своего словарного запаса. Они отличаются в *паттернах частоты* — некоторые слова более распространены в одном жанре, чем в другом, — но есть огромное перекрытие. Это создаёт нечёткие, проницаемые границы.

**Подумайте об этом как о цветах.** Красный и синий — разные цвета, но между ними есть целый спектр — фиолетовый, violet, magenta. В какой точке красный становится фиолетовым? Это не острая граница. Жанры подобны — они смешиваются друг в друга.

---

## Слайд 14: Теоретические импликации

**Позвольте мне stepping back и обсудить более широкие теоретические implications.**

**Первая имплицитность: Жанр как градиент, а не дискретный.** Наше эмпирическое свидетельство показывает систематическую путаницу между жанрами. Модели не делают случайных ошибок — они делают ошибки, которые отражают реальные структурные сходства. Это поддерживает теоретические взгляды на жанр как прототипический и нечёткий, а не категориальный.

**Вторая имплицитность: Лексическая первичность в классификации жанров.** Факт, что TF-IDF выполняет почти так же хорошо, как BERT, говорит нам, что выбор слов — основной сигнал. Контекст, синтаксис и глубокая семантика добавляют ценность, но большая часть жанровой информации в *каких словах* используются, а не *как они комбинированы*.

**Третья имплицитность: Несогласие моделей как метрика неопределённости.** Когда все три модели соглашаются, они почти всегда правильны. Когда они не соглашаются, это часто действительно неоднозначный случай. Это предполагает, что мы можем использовать несогласие моделей как способ флагировать статьи, которые нуждаются в человеческом review.

Это имеет практические приложения: представьте автоматизированную систему, которая классифицирует статьи по жанру. Большинство статей простые и классифицируются правильно всеми моделями. Но для тех 27%, где модели не соглашаются, система может флагировать эти для человеческого внимания. Это создаёт систему с человеком в цикле, которая является и эффективной, и точной.

---

## Слайд 15: Общая сводка результатов

**Позвольте мне резюмировать общую производительность моделей.**

На первом месте BERT достигает 87.64% точности. Это контекстуальная репрезентация, использующая глубокие нейронные сети с 110 миллионами параметров.

На втором месте TF-IDF достигает 86.58% — всего на 1.06% позади BERT. Это простой метод подсчёта слов без понимания контекста или семантики.

На третьем месте Linguistic признаки достигают 65% точности. Это изощрённый набор рукотворных признаков, основанный на лингвистической теории.

**Сюрприз здесь в том, что второе место настолько близко к первому.** Общепринятая мудрость в NLP в том, что большие, более сложные модели всегда выполняют лучше. Но для классификации жанров простой подход почти так же хорош, как сложный.

**И настоящий сюрприз в том, что третье место настолько далеко позади.** Лингвистические признаки основаны на тщательном анализе того, как работает язык. Но они просто не захватывают то, что важно для классификации жанров.

---

## Слайд 16: Результаты тестирования гипотез

**Вернёмся к нашим четырём гипотезам и посмотрим, что показывает свидетельство.**

**H1: Контекстуальные модели превосходят лексические модели.** Это **подтверждено частично**. BERT действительно превосходит TF-IDF, но только на 1.06%. Улучшение реальное и статистически значимое, но оно крошечное по сравнению с тем, что мы могли бы ожидать. Контекстуальные репрезентации добавляют ценность, но не много.

**H2: Жанровые границы градиентны.** Это **подтверждено**. Мы нашли 27.8% гибридных статей, где модели не соглашаются. Мы нашли систематические паттерны путаницы, которые отражают структурные сходства между жанрами. Свидетельство сильно поддерживает градиентные границы.

**H3: Лексический выбор — основной жанровый маркер.** Это **подтверждено**. TF-IDF захватывает 99% сигнала, который захватывает BERT. Это показывает, что выбор слов подавляюще самый важный фактор.

**H4: Синтаксические признаки менее информативны, чем лексические.** Это **подтверждено**. Лингвистическая модель выполняет на 21-22% хуже, чем лексические модели. Синтаксис и дискурсивная структура вторичны словарному запасу.

---

## Слайд 17: Заключение

**Позвольте мне резюмировать наши главные вклады.**

**Первый вклад: Эмпирическое свидетельство градиентных жанровых границ.** Мы показали, что ошибки моделей не случайны, а систематичны, раскрывая реальные структурные аффинности между жанрами. Около 28% статей занимают пограничные зоны, где жанр действительно неоднозначен.

**Второй вклад: Демонстрация лексической первичности.** Мы показали, что 99% жанрового сигнала в выборе слов. Это бросает вызов предположению, что сложные контекстуальные репрезентации всегда необходимы для задач NLP.

**Третий вклад: Несогласие моделей как метрика неопределённости.** Когда модели соглашаются, они почти всегда правильны. Когда они не соглашаются, это часто указывает на настоящую неоднозначность. Это предоставляет практический метод для флагирования трудных случаев.

**Четвёртый вклад: Анализ внимания раскрывает жанрово-специфические маркеры.** Мы открыли чёрный ящик BERT и показали, что он учится уделять внимание жанро-диагностическому словарю, подтверждая, что лексические паттерны — ключ.

**Практический impact:** Для производственных систем TF-IDF предлагает лучший баланс точности-сложности. Он простой, быстрый и достигает почти state-of-the-art производительности. Маленькие улучшения BERT могут не оправдывать его вычислительные затраты.

И согласие моделей может быть использовано для создания систем с человеком в цикле, которые являются и эффективными, и точными.

---

## Слайд 18: Будущая работа

**Есть несколько направлений для будущих исследований.**

**На статистической стороне**, мы уже завершили тест McNemar'а, bootstrap доверительные интервалы и Cohen's Kappa. Но мы не сделали исследования меж-аннотаторского согласия — измерение того, как часто люди соглашаются на жанровые метки. Это дало бы нам человеческий baseline для сравнения наших моделей против.

**На эмпирической стороне**, мы могли бы применить наши методы к другим языкам и другим датасетам. Обобщаются ли наши находки? Являются ли жанровые границы в российских газетах также градиентными? Являются ли онлайн-жанры (like блог-посты) аналогично нечёткими?

**На методологической стороне**, мы могли бы сделать более детальный анализ внимания. Мы смотрели на последние 6 слоёв, но мы могли бы исследовать все 12. Мы могли бы анализировать отдельные головы внимания, чтобы увидеть, что учится каждая. Мы могли бы использовать другие методы интерпретабельности like SHAP или LIME.

**На теоретической стороне**, нам нужно лучше понять, что представляют собой эти 27.8% гибридных статей. Это проблема с нашими метками? Или они представляют настоящую жанровую эволюцию? В цифровую эпоху жанры смешиваются и эволюционируют. Может быть, наши жёсткие категории не подходят больше к реальности.

---

## Слайд 19: Спасибо

**Спасибо за внимание. Я рад ответить на вопросы.**

Чтобы резюмировать наше ключевое сообщение: **Ошибки классификации жанров — это не провалы — это инсайты.** Изучая то, что модели получают неправильно, мы узнаём о нечёткой, градиентной природе жанровых категорий. И сравнивая разные типы репрезентаций, мы узнаём, что выбор слов важнее всего — находка, которая бросает вызов предположению, что большие, более сложные модели всегда лучше.

---

# Детальные объяснения (Для справки спикера)

## Глубокое погружение: Почему внимание важно (Детальное объяснение для спикера)

**Что такое внимание на самом деле?**

Представьте, что вы читаете это предложение:
> "The cat sat on the mat because it was comfortable."

Когда вы читаете "it", ваш мозг автоматически смотрит назад, чтобы понять, на что ссылается "it". Это "the cat" или "the mat"? Вы знаете, что это "the mat", потому что маты — это то, что обычно описывается как комфортное, а не кошки. Этот мыслительный процесс — соединение "it" с "mat" — это **внимание**.

У BERT 144 головы внимания (12 слоёв × 12 голов на слой). Каждая голова учится фокусироваться на разных типах отношений:

- Некоторые головы соединяют местоимения с их референтами (как "it" → "mat")
- Некоторые головы соединяют прилагательные с существительными ("comfortable" → "mat")
- Некоторые головы соединяют глаголы с их объектами ("sat" → "mat")
- Некоторые головы учат долгосрочные зависимости через предложения

Когда мы обучаем BERT для классификации жанров, эти головы внимания настраиваются, чтобы фокусироваться на словах, которые важны для жанра.

**Как мы анализируем внимание?**

Для каждого слова в тексте мы вычисляем, сколько внимания оно получает через все головы и слои. Затем мы ранжируем слова по скору внимания и смотрим на топ-слова.

Например, если мы видим, что "boris", "johnson", "downing" последовательно получают высокое внимание в аналитических статьях, это значит, что BERT научился тому, что эти политические слова диагностические для аналитического жанра.

**Почему это важно?**

Это делает BERT интерпретируемым. Вместо того чтобы говорить "BERT работает по магии", мы можем сказать "BERT работает, потому что он учится фокусироваться на жанрово-специфическом словаре".

И это показывает, что BERT и TF-IDF делают что-то похожее — идентифицируют важные слова. Разница в том, что TF-IDF механически считает слова, в то время как BERT учится уделять внимание словам чувствительным к контексту способом.

---

## Глубокое погружение: Почему тест McNemar'а (Детальное объяснение для спикера)

**Проблема с простым сравнением точности**

Если у модели A 87% точности, а у модели B 86% точности, можем ли мы сказать, что модель A лучше? Не обязательно — это может быть из-за случайного шанса.

**Почему не использовать t-test?**

T-test предполагает, что выборки независимы. Но здесь обе модели тестируются на *одном и том же* датасете. Их предсказания коррелированы. Нам нужен тест для paired данных.

**Тест McNemar'а разработан именно для этой ситуации**

Мы создаём contingency table 2×2:

|                     | Модель B Права | Модель B Неправа |
|---------------------|----------------|------------------|
| **Модель A Права**  | a              | b                |
| **Модель A Неправа**| c              | d                |

- a: обе модели правильны
- b: A права, B неправильна
- c: A неправильна, B права
- d: обе неправильны

Тест McNemar'а фокусируется только на b и c — discordant парах, где модели не соглашаются. Он спрашивает: Значительно ли b отличается от c?

Тестовая statistic: χ² = (|b - c| - 1)² / (b + c)

Если b = c, тогда χ² = 0, значит никакого значимого различия.
Если b >> c, тогда χ² большая, значит модель A significantly лучше.

**Почему вычесть 1?**

Это "continuity correction" — это делает тест более консервативным, немного уменьшая statistic. Это как запас безопасности.

**Интерпретация p-values**

- p < 0.05: значимо (есть реальная разница)
- p < 0.01: очень значимо
- p < 0.001: высоко значимо (почти наверняка реальная разница)

Наши p-values все < 0.001, что означает, что различия реальны и робастны.

---

## Глубокое погружение: Почему bootstrap доверительные интервалы (Детальное объяснение для спикера)

**Проблема с single-point оценками**

Когда мы говорим "BERT имеет 87.64% точности," это просто оценка, основанная на одном конкретном тестовом наборе. Если бы у нас был другой тестовый набор, мы могли бы получить немного другое число.

**Традиционный подход: Параметрические доверительные интервалы**

Традиционная статистика вычислила бы доверительный интервал, используя формулы, которые предполагают, что данные следуют normal distribution (колокол). Формула:

CI = mean ± 1.96 × (standard deviation / √n)

Но это предположение часто неправильное! Точность не всегда следует normal distribution, особенно для маленьких датасетов.

**Bootstrap подход: Никаких предположений**

Bootstrap умный, потому что он не делает предположений о распределениях.

Вот как это работает:

1. У нас есть тестовый набор с 5,000 статей.
2. Мы создаём новый "bootstrap sample" случайно выбирая 5,000 статей *с заменой*.
   - Это значит, что мы можем выбрать статью #5 три раза и статью #17 ноль раз
3. Мы вычисляем точность на этом bootstrap sample.
4. Мы повторяем это 1,000 раз, получая 1,000 значений точности.
5. Мы берём 2.5-й персентиль и 97.5-й персентиль этих 1,000 значений.

**Почему с заменой?**

Сэмплирование с заменой создаёт вариацию. Каждый bootstrap sample немного отличается, имитируя то, что случилось бы, если бы мы собрали новые тестовые данные. Это даёт нам распределение точностей.

**Интерпретация CI**

Если 95% CI BERT — это [92.02%, 93.42%], это значит: "Мы на 95% уверены, что истинная точность BERT на популяции всех статей Guardian между 92.02% и 93.42%."

**Почему непересекающиеся CI важны**

Если CI модели A — это [85%, 87%], а CI модели B — это [89%, 91%], они не пересекаются. Это значит, что модель B definitively лучше — нет перекрытия в plausiblera диапазонах.

Наши CI не пересекаются, подтверждая, что различия в производительности реальны.

---

## Глубокое погружение: Почему Cohen's Kappa (Детальное объяснение для спикера)

**Проблема с простым согласием**

Представьте двух людей, размечающих статьи жанрами. Они соглашаются на 80% статей. Звучит хорошо, да?

Но подождите — с 5 жанрами случайное угадывание дало бы 20% согласия (1/5 = 0.20). Так что часть из этих 80% согласия может быть просто шансом.

**Cohen's Kappa скорректирует на шанс**

Формула:
κ = (p_o - p_e) / (1 - p_e)

Где:
- p_o = наблюдаемое согласие (то, что мы видим фактически)
- p_e = ожидаемое согласие по шансу
- κ = Kappa score

Если κ = 0, согласие точно то, что мы ожидали бы по шансу.
Если κ = 1, согласие идеально (лучше, чем шанс).
Если κ < 0, согласие хуже, чем шанс!

**Пример расчёта**

Предположим, TF-IDF и BERT соглашаются на 85% статей (p_o = 0.85).

По шансу мы ожидали бы, что они соглашаются около 20% времени (p_e = 0.20).

κ = (0.85 - 0.20) / (1 - 0.20) = 0.65 / 0.80 = 0.8125

Это существенное согласие!

**Интерпретация значений Kappa (шкала Landis-Koch)**

- 0.81 - 1.00: Почти идеальное согласие
- 0.61 - 0.80: Существенное согласие ← Мы здесь
- 0.41 - 0.60: Умеренное согласие
- 0.21 - 0.40: Справедливое согласие
- 0.00 - 0.20: Слабое согласие
- < 0.00: Плохое согласие (хуже, чем шанс!)

**Что означают наши значения Kappa**

Наши значения Kappa (0.73 - 0.83) находятся в диапазоне "substantial". Это значит:

1. Модели соглашаются гораздо больше, чем ожидалось бы по шансу
2. Они захватывают один и тот же базовый сигнал
3. Жанр — это реальное, детектируемое явление (не случайный шум)

**Почему это важно**

High Kappa значит, что мы можем доверять тому, что есть последовательное понятие жанра, которое разные модели могут обнаружить. Это не как у каждой модели своё мнение — они в основном соглашаются.

Несогласия (Kappa < 1.0) — это интересные случаи — пограничные случаи, где жанр неоднозначен.

---

**Конец документа речи**
